{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f7bcf4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Estructura del Proyecto\n",
    "- **Entrada**: Diccionario PDF bilingüe (Quechua-Español / Español-Quechua)\n",
    "- **Salida**: Archivos JSON estructurados + librería Python para consultas\n",
    "\n",
    "## Tareas\n",
    "1. Extracción de texto crudo del PDF\n",
    "2. Identificación y separación de secciones\n",
    "3. Diseño e implementación de parsers\n",
    "4. Generación de archivos JSON estructurados\n",
    "5. Desarrollo de librería de utilidades\n",
    "6. Validación y testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07dfd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías instaladas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías necesarias\n",
    "# !pip install PyMuPDF pdfplumber regex\n",
    "\n",
    "# Importaciones\n",
    "import fitz  \n",
    "import pdfplumber\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Librerías instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940bb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo texto del PDF...\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extraído: 1930001 caracteres\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extraído: 1930001 caracteres\n"
     ]
    }
   ],
   "source": [
    "# TAREA 1 y 2: Extracción de texto del PDF y separación de secciones\n",
    "\n",
    "def extraer_texto_pdf(ruta_pdf: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae texto crudo del PDF preservando estructura básica sin alteraciones\n",
    "    \"\"\"\n",
    "    texto_completo = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Usar PyMuPDF para extracción\n",
    "        doc = fitz.open(ruta_pdf)\n",
    "        \n",
    "        for pagina_num in range(doc.page_count):\n",
    "            pagina = doc[pagina_num]\n",
    "            texto = pagina.get_text()\n",
    "            texto_completo += texto\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error con PyMuPDF: {e}\")\n",
    "        \n",
    "        # Alternativa con pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(ruta_pdf) as pdf:\n",
    "                for pagina in pdf.pages:\n",
    "                    texto = pagina.extract_text()\n",
    "                    if texto:\n",
    "                        texto_completo += texto\n",
    "        except Exception as e2:\n",
    "            print(f\"Error con pdfplumber: {e2}\")\n",
    "            return \"\"\n",
    "    \n",
    "    return texto_completo\n",
    "\n",
    "def guardar_texto_crudo(texto: str, ruta_salida: str = \"diccionario_raw.txt\"):\n",
    "    \"\"\"\n",
    "    Guarda el texto crudo extraído del PDF\n",
    "    \"\"\"\n",
    "    with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "        f.write(texto)\n",
    "    print(f\"Texto crudo guardado en: {ruta_salida}\")\n",
    "\n",
    "# Ejecutar extracción\n",
    "ruta_pdf = \"diccionario-qeswa-academia-mayor.pdf\"\n",
    "if os.path.exists(ruta_pdf):\n",
    "    print(\"Extrayendo texto del PDF...\")\n",
    "    texto_crudo = extraer_texto_pdf(ruta_pdf)\n",
    "    guardar_texto_crudo(texto_crudo)\n",
    "    print(f\"Texto extraído: {len(texto_crudo)} caracteres\")\n",
    "else:\n",
    "    print(f\"Archivo PDF no encontrado: {ruta_pdf}\")\n",
    "    print(\"Por favor, asegúrese de que el archivo esté en el directorio correcto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando secciones del diccionario...\n",
      "Sección Quechua-Español: 48883 líneas\n",
      "Sección Español-Quechua: 9264 líneas\n",
      "Archivos guardados correctamente\n",
      "Sección Quechua-Español: 48883 líneas\n",
      "Sección Español-Quechua: 9264 líneas\n",
      "Archivos guardados correctamente\n"
     ]
    }
   ],
   "source": [
    "# TAREA 3: Extracción y limpieza de secciones del diccionario\n",
    "\n",
    "def limpiar_encabezados_y_patrones(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando encabezados de página, títulos, letras de sección y otros patrones no deseados.\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    lineas_limpias = []\n",
    "    \n",
    "    for linea in lineas:\n",
    "        linea_original = linea\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Ignorar líneas vacías\n",
    "        if not linea:\n",
    "            continue\n",
    "        \n",
    "        # PATRONES A ELIMINAR:\n",
    "        \n",
    "        # 1. Números de página junto con DICCIONARIO\n",
    "        if re.match(r'^\\d+$', linea) or 'DICCIONARIO' in linea.upper():\n",
    "            continue\n",
    "        \n",
    "        # 2. Símbolos decorativos como ◄●►\n",
    "        if re.match(r'^[◄●►\\-=\\*\\+\\s]+$', linea):\n",
    "            continue\n",
    "        \n",
    "        # 3. Título SIMI TAQE\n",
    "        if 'SIMI TAQE' in linea.upper():\n",
    "            continue\n",
    "        \n",
    "        # 4. Letras sueltas de sección (una sola letra mayúscula en su propia línea)\n",
    "        if re.match(r'^[A-Z]$', linea):\n",
    "            continue\n",
    "        \n",
    "        # 5. Encabezados típicos de PDF\n",
    "        if any(patron in linea.upper() for patron in [\n",
    "            'PÁGINA', 'CAPÍTULO', 'SECCIÓN', 'ÍNDICE', 'CONTENIDO',\n",
    "            'DICCIONARIO QUECHUA', 'ESPAÑOL - QUECHUA', 'QUECHUA - ESPAÑOL'\n",
    "        ]):\n",
    "            continue\n",
    "        \n",
    "        # 6. Líneas que son solo números o símbolos\n",
    "        if re.match(r'^[\\d\\s\\-\\.\\*\\+◄●►]+$', linea):\n",
    "            continue\n",
    "        \n",
    "        # 7. Líneas muy cortas que no parecen ser contenido útil \n",
    "        if len(linea) < 3:\n",
    "            continue\n",
    "        \n",
    "        # 8. Patrones específicos de encabezados de página\n",
    "        if re.match(r'^\\d+\\s*◄●►\\s*$', linea) or re.match(r'^◄●►\\s*\\d+\\s*$', linea):\n",
    "            continue\n",
    "        \n",
    "        # Si la línea pasó todos los filtros, mantenerla\n",
    "        lineas_limpias.append(linea_original)\n",
    "    \n",
    "    return '\\n'.join(lineas_limpias)\n",
    "\n",
    "def separar_secciones_por_lineas(texto: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Separa las secciones usando los números de línea conocidos y aplica limpieza de encabezados\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    seccion_qe_lineas = lineas[1325:50998]\n",
    "    \n",
    "    # Limpiar texto extra al final de la sección Quechua-Español\n",
    "    seccion_qe_contenido = []\n",
    "    for linea in seccion_qe_lineas:\n",
    "        if \"ESPAÑOL - QUECHUA\" in linea:\n",
    "            break\n",
    "        seccion_qe_contenido.append(linea)\n",
    "    \n",
    "    seccion_eq_contenido = []\n",
    "    for i in range(50998, len(lineas)):\n",
    "        linea = lineas[i].strip()\n",
    "        if (\"DICCIONARIO QUECHUA – ESPAÑOL – QUECHUA\" in linea or \n",
    "            \"se terminó de imprimir\" in linea or \n",
    "            \"talleres gráficos\" in linea):\n",
    "            break\n",
    "        seccion_eq_contenido.append(lineas[i])\n",
    "    \n",
    "    # Aplicar limpieza de encabezados y patrones no deseados\n",
    "    seccion_qe_texto = '\\n'.join(seccion_qe_contenido)\n",
    "    seccion_eq_texto = '\\n'.join(seccion_eq_contenido)\n",
    "    \n",
    "    seccion_qe_limpia = limpiar_encabezados_y_patrones(seccion_qe_texto)\n",
    "    seccion_eq_limpia = limpiar_encabezados_y_patrones(seccion_eq_texto)\n",
    "    \n",
    "    return seccion_qe_limpia, seccion_eq_limpia\n",
    "\n",
    "# Ejecutar extracción y limpieza de secciones\n",
    "if os.path.exists(\"diccionario_raw.txt\"):\n",
    "    with open(\"diccionario_raw.txt\", 'r', encoding='utf-8') as f:\n",
    "        texto_completo = f.read()\n",
    "    \n",
    "    print(\"Procesando secciones del diccionario...\")\n",
    "    \n",
    "    seccion_qe, seccion_eq = separar_secciones_por_lineas(texto_completo)\n",
    "    \n",
    "    # Contar líneas aproximadas para verificar el contenido\n",
    "    lineas_qe = [l for l in seccion_qe.split('\\n') if l.strip()]\n",
    "    lineas_eq = [l for l in seccion_eq.split('\\n') if l.strip()]\n",
    "    \n",
    "    print(f\"Sección Quechua-Español: {len(lineas_qe)} líneas\")\n",
    "    print(f\"Sección Español-Quechua: {len(lineas_eq)} líneas\")\n",
    "    \n",
    "    # Guardar archivos limpios\n",
    "    with open(\"seccion_quechua_espanol.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_qe)\n",
    "    \n",
    "    with open(\"seccion_espanol_quechua.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_eq)\n",
    "    \n",
    "    print(\"Archivos guardados correctamente\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: archivo diccionario_raw.txt no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdafa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsers implementados correctamente\n"
     ]
    }
   ],
   "source": [
    "# TAREA 4: Diseño e implementación de parsers\n",
    "\n",
    "def extraer_entradas_completas(texto: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrae lemas completos del texto.\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    entradas = []\n",
    "    entrada_actual = \"\"\n",
    "\n",
    "    # Patrón que detecta el inicio de una nueva entrada\n",
    "    patron_inicio = (\n",
    "        r\"^([a-zA-ZÀ-ÿñÑ!¡¿?'\\-.,–\\s]+)[\\.\\!]\\s*\"\n",
    "        r\"((s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.)(\\s*y\\s*(s\\.|adj\\.|v\\.))?)\"\n",
    "        r\"(\\s*V\\.[A-ZÁÉÍÓÚÑ;,\\s']+)?\"\n",
    "    )\n",
    "\n",
    "    for linea in lineas:\n",
    "        linea = linea.strip()\n",
    "        if re.match(patron_inicio, linea, re.IGNORECASE):\n",
    "            if entrada_actual.strip():\n",
    "                entradas.append(entrada_actual.strip())\n",
    "            entrada_actual = linea\n",
    "        else:\n",
    "            if entrada_actual:\n",
    "                entrada_actual += \" \" + linea\n",
    "\n",
    "    if entrada_actual.strip():\n",
    "        entradas.append(entrada_actual.strip())\n",
    "\n",
    "    return entradas\n",
    "\n",
    "def parsear_entrada(entrada_texto: str, abreviaturas: Dict, es_espanol: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Este módulo implementa toda la lógica para parsear una entrada del diccionario\n",
    "    y devuelve una lista de diccionarios con la estructura requerida.\n",
    "    A continuación se enumera los pasos seguidos.\n",
    "    \"\"\"\n",
    "    if len(entrada_texto) < 10:\n",
    "        return []\n",
    "    \n",
    "    # 1. EXTRAER LEMA\n",
    "    match_lema = re.match(r'^([a-zA-ZÀ-ÿñÑ!¡¿?\\'\\-.,–\\s]+?)[\\.!]\\s', entrada_texto)\n",
    "    if not match_lema:\n",
    "        return []\n",
    "    \n",
    "    lema = match_lema.group(1).strip()\n",
    "    if ' ' not in lema and not lema[0].isupper():\n",
    "        lema = lema.lower()\n",
    "    \n",
    "    # Extraer resto del texto después del lema\n",
    "    resto = entrada_texto[len(match_lema.group(0)):].strip()\n",
    "    \n",
    "    # Filtro para español: evitar palabras quechuas\n",
    "    if es_espanol:\n",
    "        if any(c in lema.lower() for c in ['k', 'w', 'q']) and 'qu' not in lema.lower():\n",
    "            return []\n",
    "    \n",
    "    # Obtener listas de abreviaturas\n",
    "    categorias_gramaticales = abreviaturas.get('categorias_gramaticales', [])\n",
    "    campos_semanticos = abreviaturas.get('campos_semanticos', [])\n",
    "    dialectales = abreviaturas.get('dialectales', [])\n",
    "    \n",
    "    # 2. EXTRAER CATEGORÍA GRAMATICAL \n",
    "    categoria = ''\n",
    "    for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "        if resto.startswith(cat):\n",
    "            categoria = cat\n",
    "            resto = resto[len(cat):].strip()\n",
    "            break\n",
    "    \n",
    "    if not categoria:\n",
    "        return []\n",
    "    \n",
    "    # 3. DETECTAR TODOS LOS CAMPOS SEMÁNTICOS EN TODA LA DEFINICIÓN\n",
    "    campos_encontrados = []\n",
    "    texto_busqueda = resto\n",
    "    \n",
    "    # Buscar TODOS los campos semánticos en todo el texto, no solo al inicio\n",
    "    for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "        # Buscar en diferentes posiciones: inicio, después de punto, en medio\n",
    "        patrones_busqueda = [\n",
    "            f'^{re.escape(campo)}\\\\s',          # Al inicio\n",
    "            f'\\\\s{re.escape(campo)}\\\\s',        # En medio del texto\n",
    "            f'\\\\.\\\\s*{re.escape(campo)}\\\\s',    # Después de punto\n",
    "            f'^{re.escape(campo)}\\\\.',          # Al inicio con punto\n",
    "            f'\\\\s{re.escape(campo)}\\\\.'         # En medio con punto\n",
    "        ]\n",
    "        \n",
    "        for patron in patrones_busqueda:\n",
    "            if re.search(patron, texto_busqueda):\n",
    "                if campo not in campos_encontrados:\n",
    "                    campos_encontrados.append(campo)\n",
    "                break\n",
    "    \n",
    "    # 4. PROCESAMIENTO MEJORADO DE CONTENIDO DESPUÉS DE BARRAS ||\n",
    "    if '||' in resto:\n",
    "        partes_separadas = [a.strip() for a in resto.split('||') if a.strip()]\n",
    "    else:\n",
    "        partes_separadas = [resto]\n",
    "    \n",
    "    # Estructura única de entrada que iremos completando\n",
    "    entrada = {\n",
    "        'lema': lema,\n",
    "        'categoria_gramatical': [categoria],\n",
    "        'campo_semantico': campos_encontrados.copy(),\n",
    "        'definicion': '',\n",
    "        'variantes_dialectales': {},\n",
    "        'sinonimos': [],\n",
    "        'antonimos': [],  # NUEVA LÍNEA\n",
    "        'ejemplos': []\n",
    "    }\n",
    "    \n",
    "    # Lista para acumular todas las definiciones\n",
    "    todas_definiciones = []\n",
    "    \n",
    "    for i, parte in enumerate(partes_separadas):\n",
    "        texto = parte.strip()\n",
    "        \n",
    "        # 5. DETECTAR REMISIONES TIPO \"V. LEMA\" \n",
    "        match_remision = re.search(r'\\bV\\.\\s+([A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*)', texto)\n",
    "        if match_remision:\n",
    "            remite_a = match_remision.group(1).strip().lower()\n",
    "            entrada['remite_a'] = remite_a\n",
    "            texto = re.sub(r'\\bV\\.\\s+[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*', '', texto).strip()\n",
    "        \n",
    "        # 6. DETECTAR CATEGORÍA GRAMATICAL ADICIONAL PARA CASOS COMO \"|| adj. insult.\"\n",
    "        categoria_adicional = ''\n",
    "        for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "            if texto.startswith(cat):\n",
    "                categoria_adicional = cat\n",
    "                if cat not in entrada['categoria_gramatical']:\n",
    "                    entrada['categoria_gramatical'].append(cat)\n",
    "                texto = texto[len(cat):].strip()\n",
    "                break\n",
    "        \n",
    "        # 7. DETECTAR CAMPOS SEMÁNTICOS ADICIONALES PARA CASOS COMO \"|| Agri.\"\n",
    "        campos_adicionales = []\n",
    "        for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "            if texto.startswith(campo):\n",
    "                campos_adicionales.append(campo)\n",
    "                if campo not in entrada['campo_semantico']:\n",
    "                    entrada['campo_semantico'].append(campo)\n",
    "                texto = texto[len(campo):].strip()\n",
    "                break\n",
    "        \n",
    "        # 8. DETERMINAR TIPO DE CONTENIDO DESPUÉS DE ||\n",
    "        es_dialectal = False\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            if (f'{dialecto_sin_punto}:' in texto or \n",
    "                texto.startswith(dialecto_sin_punto + ':') or\n",
    "                f' {dialecto_sin_punto}:' in texto):\n",
    "                es_dialectal = True\n",
    "                break\n",
    "        \n",
    "        # 9. EXTRAER EJEMPLOS \n",
    "        if 'EJEM:' in texto:\n",
    "            pos_ejem = texto.find('EJEM:')\n",
    "            texto_antes = texto[:pos_ejem].strip()\n",
    "            texto_ejem = texto[pos_ejem + 5:].strip()\n",
    "            \n",
    "            # Encontrar final del ejemplo\n",
    "            fin_ejemplo = len(texto_ejem)\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                pos_dialecto = texto_ejem.find(f'{dialecto_sin_punto}:')\n",
    "                if pos_dialecto != -1 and pos_dialecto < fin_ejemplo:\n",
    "                    fin_ejemplo = pos_dialecto\n",
    "            \n",
    "            ejemplo_texto = texto_ejem[:fin_ejemplo].strip(' .,!')\n",
    "            if ejemplo_texto and ejemplo_texto not in entrada['ejemplos']:\n",
    "                entrada['ejemplos'].append(ejemplo_texto)\n",
    "            \n",
    "            # Continuar con el resto después del ejemplo\n",
    "            resto_ejemplo = texto_ejem[fin_ejemplo:].strip()\n",
    "            texto = texto_antes + ' ' + resto_ejemplo\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 8. EXTRAER SINÓNIMOS \n",
    "        if 'SINÓN:' in texto:\n",
    "            pos_sinon = texto.find('SINÓN:')\n",
    "            texto_antes = texto[:pos_sinon].strip()\n",
    "            texto_sinon = texto[pos_sinon + 6:].strip()\n",
    "            \n",
    "            fin_sinon = len(texto_sinon)\n",
    "            \n",
    "            # Buscar primer punto seguido de mayúscula\n",
    "            for j, char in enumerate(texto_sinon):\n",
    "                if char == '.' and j + 1 < len(texto_sinon):\n",
    "                    siguiente = texto_sinon[j + 1:j + 10].strip()\n",
    "                    if siguiente and siguiente[0].isupper():\n",
    "                        fin_sinon = j + 1\n",
    "                        break\n",
    "            \n",
    "            # Buscar abreviaturas dialectales que interrumpen sinónimos\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                for patron in [f' {dialecto_sin_punto}:', f' {dialecto_sin_punto} ']:\n",
    "                    pos = texto_sinon.find(patron)\n",
    "                    if pos != -1 and pos < fin_sinon:\n",
    "                        fin_sinon = pos\n",
    "            \n",
    "            sinon_texto = texto_sinon[:fin_sinon].strip(' .,!')\n",
    "            if sinon_texto:\n",
    "                sinonimos = [s.strip() for s in sinon_texto.split(',') if s.strip()]\n",
    "                entrada['sinonimos'] = [re.sub(r'[^\\w\\sñÑáéíóúÁÉÍÓÚ]', '', s).strip() \n",
    "                                      for s in sinonimos if len(s.strip()) > 1]\n",
    "            \n",
    "            # Continuar con el resto después de los sinónimos\n",
    "            resto_despues_sinon = texto_sinon[fin_sinon:].strip()\n",
    "            texto = texto_antes + ' ' + resto_despues_sinon\n",
    "            texto = texto.strip()\n",
    "\n",
    "        # 9. EXTRAER ANTÓNIMOS  \n",
    "        if 'ANTÓN:' in texto:\n",
    "            pos_anton = texto.find('ANTÓN:')\n",
    "            texto_antes = texto[:pos_anton].strip()\n",
    "            texto_anton = texto[pos_anton + 6:].strip()\n",
    "            fin_anton = len(texto_anton)\n",
    "            for j, char in enumerate(texto_anton):\n",
    "                if char == '.' and j + 1 < len(texto_anton):\n",
    "                    siguiente = texto_anton[j + 1:j + 10].strip()\n",
    "                    if siguiente and siguiente[0].isupper():\n",
    "                        fin_anton = j + 1\n",
    "                        break\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                for patron in [f' {dialecto_sin_punto}:', f' {dialecto_sin_punto} ']:\n",
    "                    pos = texto_anton.find(patron)\n",
    "                    if pos != -1 and pos < fin_anton:\n",
    "                        fin_anton = pos\n",
    "            anton_texto = texto_anton[:fin_anton].strip(' .,!')\n",
    "            if anton_texto:\n",
    "                antonimos = [s.strip() for s in anton_texto.split(',') if s.strip()]\n",
    "                entrada['antonimos'] = [re.sub(r'[^\\w\\sñÑáéíóúÁÉÍÓÚ]', '', s).strip() \n",
    "                                        for s in antonimos if len(s.strip()) > 1]\n",
    "            resto_despues_anton = texto_anton[fin_anton:].strip()\n",
    "            texto = texto_antes + ' ' + resto_despues_anton\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 10. EXTRAER VARIANTES DIALECTALES \n",
    "        texto_limpio = texto\n",
    "        dialectales_ordenados = sorted(dialectales, key=len, reverse=True)\n",
    "        \n",
    "        for dialecto in dialectales_ordenados:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            \n",
    "            # Buscar patrón al inicio o después de espacios\n",
    "            patrones_dialectal = [\n",
    "                f'{dialecto_sin_punto}:',      # Al inicio: \"Arg:\"\n",
    "                f' {dialecto_sin_punto}:',     # En medio: \" Arg:\"\n",
    "                f'^{dialecto_sin_punto}:'      # Exactamente al inicio\n",
    "            ]\n",
    "            \n",
    "            pos_encontrada = -1\n",
    "            patron_usado = ''\n",
    "            \n",
    "            for patron in patrones_dialectal:\n",
    "                if patron.startswith('^'):\n",
    "                    # Patrón de inicio exacto\n",
    "                    if texto_limpio.startswith(patron[1:]):\n",
    "                        pos_encontrada = 0\n",
    "                        patron_usado = patron[1:]\n",
    "                        break\n",
    "                else:\n",
    "                    pos = texto_limpio.find(patron)\n",
    "                    if pos != -1:\n",
    "                        pos_encontrada = pos\n",
    "                        patron_usado = patron\n",
    "                        break\n",
    "            \n",
    "            if pos_encontrada != -1:\n",
    "                texto_antes = texto_limpio[:pos_encontrada].strip()\n",
    "                texto_despues = texto_limpio[pos_encontrada + len(patron_usado):].strip()\n",
    "                \n",
    "                # Encontrar final de variantes dialectales\n",
    "                fin_dialectal = len(texto_despues)\n",
    "                \n",
    "                # Buscar próximo dialecto\n",
    "                for otro_dialecto in dialectales_ordenados:\n",
    "                    if otro_dialecto != dialecto:\n",
    "                        otro_sin_punto = otro_dialecto.rstrip('.')\n",
    "                        for patron_otro in [f'{otro_sin_punto}:', f' {otro_sin_punto}:']:\n",
    "                            pos_otro = texto_despues.find(patron_otro)\n",
    "                            if pos_otro != -1 and pos_otro < fin_dialectal:\n",
    "                                fin_dialectal = pos_otro\n",
    "                \n",
    "                # Buscar marcadores que interrumpen\n",
    "                for marcador in [' SINÓN:', ' EJEM:', '. SINÓN:', '. EJEM:']:\n",
    "                    pos_marcador = texto_despues.find(marcador)\n",
    "                    if pos_marcador != -1 and pos_marcador < fin_dialectal:\n",
    "                        fin_dialectal = pos_marcador\n",
    "                \n",
    "                contenido_dialectal = texto_despues[:fin_dialectal].strip(' .,!')\n",
    "                \n",
    "                if contenido_dialectal:\n",
    "                    regiones_adicionales = []\n",
    "                    variantes_finales = []\n",
    "                    \n",
    "                    # Separar por dos puntos para detectar regiones comprimidas\n",
    "                    partes_dos_puntos = contenido_dialectal.split(':')\n",
    "                    \n",
    "                    if len(partes_dos_puntos) > 1:\n",
    "                        for parte in partes_dos_puntos[:-1]: \n",
    "                            parte = parte.strip()\n",
    "                            if re.match(r'^[A-Z][a-z]{2,3}$', parte):\n",
    "                                regiones_adicionales.append(parte)\n",
    "                        \n",
    "                        variantes_texto = partes_dos_puntos[-1].strip()\n",
    "                        variantes_finales = [v.strip() for v in variantes_texto.split(',') if v.strip()]\n",
    "                    else:\n",
    "                        variantes_finales = [v.strip() for v in contenido_dialectal.split(',') if v.strip()]\n",
    "                    \n",
    "                    # CREAR ESTRUCTURA DE VARIANTES DIALECTALES\n",
    "                    if '.' in dialecto and len(dialecto.split('.', 1)[1].rstrip('.')) > 0:\n",
    "                        pais, region_principal = dialecto.split('.', 1)\n",
    "                        region_principal = region_principal.rstrip('.')\n",
    "                        \n",
    "                        if pais not in entrada['variantes_dialectales']:\n",
    "                            entrada['variantes_dialectales'][pais] = {}\n",
    "                        \n",
    "                        # Agregar región principal\n",
    "                        entrada['variantes_dialectales'][pais][region_principal] = variantes_finales\n",
    "                        \n",
    "                        # Agregar regiones adicionales con las mismas variantes\n",
    "                        for region_adicional in regiones_adicionales:\n",
    "                            entrada['variantes_dialectales'][pais][region_adicional] = variantes_finales\n",
    "                            \n",
    "                    else:\n",
    "                        pais = dialecto.rstrip('.')\n",
    "                        entrada['variantes_dialectales'][pais] = variantes_finales\n",
    "                \n",
    "                # Limpiar del texto principal\n",
    "                texto_limpio = texto_antes + ' ' + texto_despues[fin_dialectal:]\n",
    "                texto_limpio = texto_limpio.strip()\n",
    "        \n",
    "        # 10. EXTRAER DEFINICIÓN, LO QUE QUEDA DESPUÉS DE LIMPIAR TODO\n",
    "        definicion = texto_limpio\n",
    "        \n",
    "        # REGLA 1: Limpiar todos los campos semánticos detectados de la definición\n",
    "        for campo in campos_encontrados:\n",
    "            # Limpiar campo al inicio de la definición\n",
    "            definicion = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion)\n",
    "            # Limpiar campo en medio de la definición\n",
    "            definicion = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion)\n",
    "            # Limpiar campo después de punto\n",
    "            definicion = re.sub(f'\\\\.\\\\s*{re.escape(campo)}\\\\s*', '. ', definicion)\n",
    "        \n",
    "        # Limpiar campos adicionales encontrados después de ||\n",
    "        for campo in campos_adicionales:\n",
    "            definicion = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion)\n",
    "            definicion = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion)\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion = re.sub(r'SINÓN:.*$', '', definicion)\n",
    "        definicion = re.sub(r'EJEM:.*$', '', definicion)\n",
    "        definicion = re.sub(r'VARIEDADES:.*$', '', definicion, flags=re.IGNORECASE)\n",
    "        definicion = re.sub(r'\\bV\\.\\s+[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*', '', definicion)\n",
    "        \n",
    "        # Limpiar dialectales residuales\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            definicion = re.sub(f'{re.escape(dialecto_sin_punto)}:.*$', '', definicion)\n",
    "        \n",
    "        # 12. EXTRAER DEFINICIÓN \n",
    "        definicion_parte = texto_limpio\n",
    "        \n",
    "        # Limpiar campos semánticos del texto\n",
    "        for campo in entrada['campo_semantico']:\n",
    "            definicion_parte = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion_parte)\n",
    "            definicion_parte = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion_parte)\n",
    "            definicion_parte = re.sub(f'\\\\.\\\\s*{re.escape(campo)}\\\\s*', '. ', definicion_parte)\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion_parte = re.sub(r'SINÓN:.*$', '', definicion_parte)\n",
    "        definicion_parte = re.sub(r'EJEM:.*$', '', definicion_parte)\n",
    "        definicion_parte = re.sub(r'VARIEDADES:.*$', '', definicion_parte, flags=re.IGNORECASE)\n",
    "        definicion_parte = re.sub(r'^\\([^)]*\\)\\.\\s*', '', definicion_parte)  # Nombres científicos solos\n",
    "        \n",
    "        # REGLA 3: Evitar contaminación y detectar nueva entrada\n",
    "        patron_nueva_entrada = r'\\b[a-zA-Z]+\\.\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.|\\bV\\.)'\n",
    "        match_nueva = re.search(patron_nueva_entrada, definicion_parte)\n",
    "        if match_nueva:\n",
    "            definicion_parte = definicion_parte[:match_nueva.start()].strip()\n",
    "        \n",
    "        definicion_parte = re.sub(r'\\s+', ' ', definicion_parte).strip(' .,!')\n",
    "        \n",
    "        # AGREGAR A DEFINICIONES TOTALES\n",
    "        if i == 0:\n",
    "            if definicion_parte:\n",
    "                todas_definiciones.append(definicion_parte)\n",
    "        else:\n",
    "            # Partes después de || incluir si hay contenido y no es solo dialectal\n",
    "            if definicion_parte and not es_dialectal:\n",
    "                todas_definiciones.append(definicion_parte)\n",
    "            elif definicion_parte and es_dialectal:\n",
    "                # Si es dialectal pero hay contenido antes del dialecto, incluirlo\n",
    "                texto_antes_dialectos = definicion_parte\n",
    "                for dialecto in dialectales:\n",
    "                    dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                    patron_corte = f'{dialecto_sin_punto}:'\n",
    "                    if patron_corte in texto_antes_dialectos:\n",
    "                        pos_corte = texto_antes_dialectos.find(patron_corte)\n",
    "                        texto_antes_dialectos = texto_antes_dialectos[:pos_corte].strip()\n",
    "                        break\n",
    "                if texto_antes_dialectos:\n",
    "                    todas_definiciones.append(texto_antes_dialectos)\n",
    "        \n",
    "    # COMBINAR TODAS LAS DEFINICIONES\n",
    "    if todas_definiciones:\n",
    "        entrada['definicion'] = '. '.join(todas_definiciones)\n",
    "    \n",
    "    # FORMATEAR CATEGORÍA GRAMATICAL\n",
    "    if len(entrada['categoria_gramatical']) == 1:\n",
    "        entrada['categoria_gramatical'] = entrada['categoria_gramatical'][0]\n",
    "    \n",
    "    # FORMATEAR CAMPO SEMÁNTICO \n",
    "    if not entrada['campo_semantico']:\n",
    "        entrada['campo_semantico'] = []\n",
    "    \n",
    "    # RETORNAR ENTRADA ÚNICA \n",
    "    if (entrada['definicion'] or entrada['sinonimos'] or \n",
    "        entrada['variantes_dialectales'] or entrada.get('remite_a')):\n",
    "        return [entrada]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def parsear_seccion_quechua_espanol(texto: str, abreviaturas: Dict[str, List[str]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parsea la sección Quechua-Español con el parser ultra-estricto\n",
    "    \"\"\"\n",
    "    entradas = []\n",
    "    entradas_texto = extraer_entradas_completas(texto)\n",
    "    print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "    for entrada_texto in entradas_texto:\n",
    "        entradas_procesadas = parsear_entrada(\n",
    "            entrada_texto,\n",
    "            abreviaturas,\n",
    "            es_espanol=False\n",
    "        )\n",
    "        if entradas_procesadas:\n",
    "            entradas.extend(entradas_procesadas)\n",
    "    return entradas\n",
    "\n",
    "\n",
    "def parsear_seccion_espanol_quechua(texto: str, abreviaturas: Dict[str, List[str]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parsea la sección Español-Quechua con el parser ultra-estricto\n",
    "    \"\"\"\n",
    "    entradas = []\n",
    "    entradas_texto = extraer_entradas_completas(texto)\n",
    "    print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "    for entrada_texto in entradas_texto:\n",
    "        entradas_procesadas = parsear_entrada(\n",
    "            entrada_texto,\n",
    "            abreviaturas,\n",
    "            es_espanol=True\n",
    "        )\n",
    "        if entradas_procesadas:\n",
    "            entradas.extend(entradas_procesadas)\n",
    "    return entradas\n",
    "\n",
    "\n",
    "print(\"Parsers implementados correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f44784e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando sección Quechua-Español...\n",
      "Encontradas 15204 entradas potenciales\n",
      "15039 entradas procesadas\n",
      "\n",
      "Procesando sección Español-Quechua...\n",
      "Encontradas 5602 entradas potenciales\n",
      "5526 entradas procesadas\n",
      "\n",
      "Guardado: quechua_espanol.json\n",
      "Guardado: espanol_quechua.json\n"
     ]
    }
   ],
   "source": [
    "# TAREA 5: Generación de archivos JSON estructurados\n",
    "\n",
    "# Carga las abreviaturas desde el archivo JSON generado anteriormente\n",
    "def cargar_abreviaturas():\n",
    "    with open(\"abreviaturas.json\", 'r', encoding='utf-8') as f:\n",
    "        abreviaturas = json.load(f)\n",
    "    return abreviaturas\n",
    "\n",
    "\n",
    "\n",
    "# Ejecuta los parsers y genera los archivos JSON según las especificaciones\n",
    "def generar_archivos_json():\n",
    "    abreviaturas = cargar_abreviaturas()\n",
    "    \n",
    "    # Procesar sección Quechua-Español\n",
    "    entradas_qe = []\n",
    "    if os.path.exists(\"seccion_quechua_espanol.txt\"):\n",
    "        print(\"\\nProcesando sección Quechua-Español...\")\n",
    "        with open(\"seccion_quechua_espanol.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_qe = f.read()\n",
    "        \n",
    "        entradas_qe = parsear_seccion_quechua_espanol(texto_qe, abreviaturas)\n",
    "        print(f\"{len(entradas_qe)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"Archivo seccion_quechua_espanol.txt no encontrado\")\n",
    "    \n",
    "    # Procesar sección Español-Quechua\n",
    "    entradas_eq = []\n",
    "    if os.path.exists(\"seccion_espanol_quechua.txt\"):\n",
    "        print(\"\\nProcesando sección Español-Quechua...\")\n",
    "        with open(\"seccion_espanol_quechua.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_eq = f.read()\n",
    "        \n",
    "        entradas_eq = parsear_seccion_espanol_quechua(texto_eq, abreviaturas)\n",
    "        print(f\"{len(entradas_eq)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"Archivo seccion_espanol_quechua.txt no encontrado\")\n",
    "    \n",
    "    # Guardar archivos JSON\n",
    "    if entradas_qe:\n",
    "        with open(\"quechua_espanol.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_qe, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nGuardado: quechua_espanol.json\")\n",
    "    \n",
    "    if entradas_eq:\n",
    "        with open(\"espanol_quechua.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_eq, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Guardado: espanol_quechua.json\")\n",
    "\n",
    "\n",
    "generar_archivos_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f23bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAREA 6: Validación y pruebas del diccionario\n",
    "from diccionario_utils import DiccionarioQuechua\n",
    "import json\n",
    "\n",
    "# Instancia global del diccionario\n",
    "dic = DiccionarioQuechua()\n",
    "\n",
    "# Normalización de campos para corregir tipo de campos en json\n",
    "def normalizar_campos(entrada):\n",
    "    # Normaliza categoria_gramatical y campo_semantico a listas de strings\n",
    "    for campo in ['categoria_gramatical', 'campo_semantico']:\n",
    "        valor = entrada.get(campo, None)\n",
    "        if isinstance(valor, list):\n",
    "            entrada[campo] = [str(v) for v in valor if isinstance(v, str)]\n",
    "        elif isinstance(valor, str):\n",
    "            entrada[campo] = [valor]\n",
    "        else:\n",
    "            entrada[campo] = []\n",
    "    return entrada\n",
    "\n",
    "# Contar entradas manualmente en archivos JSON\n",
    "def contar_manual(archivo):\n",
    "    try:\n",
    "        with open(archivo, 'r', encoding='utf-8') as f:\n",
    "            datos = json.load(f)\n",
    "            datos = [normalizar_campos(e) for e in datos]\n",
    "            return len(datos)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Validar conteos de entradas\n",
    "def validar_conteos():\n",
    "    print(\"VALIDACIÓN DE CONTEOS DE ENTRADA\")\n",
    "    try:\n",
    "        manual_qe = contar_manual(\"quechua_espanol.json\")\n",
    "        manual_eq = contar_manual(\"espanol_quechua.json\")\n",
    "        lib = dic.contar_entradas()\n",
    "\n",
    "        print(\"Quechua-Español\")\n",
    "        print(f\"  Entradas manuales en JSON: {manual_qe}\")\n",
    "        print(f\"  Entradas reportadas por la librería: {lib['quechua_espanol']}\")\n",
    "        print(f\"  Coincidencia: {'Sí' if manual_qe == lib['quechua_espanol'] else 'No'}\")\n",
    "        print()\n",
    "        print(\"Español-Quechua\")\n",
    "        print(f\"  Entradas manuales en JSON: {manual_eq}\")\n",
    "        print(f\"  Entradas reportadas por la librería: {lib['espanol_quechua']}\")\n",
    "        print(f\"  Coincidencia: {'Sí' if manual_eq == lib['espanol_quechua'] else 'No'}\")\n",
    "        print()\n",
    "        if manual_qe != lib['quechua_espanol'] or manual_eq != lib['espanol_quechua']:\n",
    "            print(\"Advertencia: Hay diferencia entre el conteo manual y el de la librería. Revise los datos y la función de carga.\")\n",
    "        else:\n",
    "            print(\"Los conteos coinciden correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error en conteos: {e}\")\n",
    "    print()\n",
    "\n",
    "# Probar busqueda de lemas\n",
    "def probar_busquedas():\n",
    "    print(\"PRUEBAS DE BÚSQUEDA DE LEMAS\")\n",
    "    lemas = [\n",
    "        (\"achalay\", \"que\"), (\"makichay\", \"que\"), (\"intuq\", \"que\"), (\"kachapuri\", \"que\"),\n",
    "        (\"achuqalla\", \"que\"), (\"Eqop\", \"que\"), (\"Pachar\", \"que\"),\n",
    "        (\"viajar\", \"esp\"), (\"cachorro\", \"esp\"), (\"ardid\", \"esp\"),\n",
    "        (\"entrevista\", \"esp\"), (\"zurcir\", \"esp\")\n",
    "    ]\n",
    "    encontrados = 0\n",
    "    for lema, tipo in lemas:\n",
    "        if tipo == \"esp\":\n",
    "            res = dic.buscar_por_espanol(lema)\n",
    "        else:\n",
    "            res = dic.buscar_por_quechua(lema)\n",
    "        # Normalizar resultados\n",
    "        res = [normalizar_campos(e) for e in res]\n",
    "        if res:\n",
    "            encontrados += 1\n",
    "            def_corta = res[0].get('definicion', '')[:60]\n",
    "            print(f\"Lema '{lema}' ({'Español' if tipo == 'esp' else 'Quechua'}): {len(res)} resultado(s). Definición: {def_corta}\")\n",
    "        else:\n",
    "            print(f\"Lema '{lema}' ({'Español' if tipo == 'esp' else 'Quechua'}): sin resultados\")\n",
    "    print(f\"Lemas encontrados: {encontrados} de {len(lemas)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Validar categorías gramaticales\n",
    "def validar_categorias():\n",
    "    print(\"CATEGORÍAS GRAMATICALES PARA QECHUA-ESPAÑOL\")\n",
    "    cats = dic.listar_categorias_gramaticales()\n",
    "    print(f\"Total categorías: {len(cats)}\")\n",
    "    for cat in sorted(cats):\n",
    "        resultados = dic.buscar_por_categoria_gramatical(cat)\n",
    "        resultados = [normalizar_campos(e) for e in resultados]\n",
    "        cnt = len(resultados)\n",
    "        print(f\"{cat}: {cnt}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Validar campos semánticos\n",
    "def validar_campos():\n",
    "    print(\"CAMPOS SEMÁNTICOS\")\n",
    "    campos = dic.listar_campos_semanticos()\n",
    "    print(f\"Total campos: {len(campos)}\")\n",
    "    for campo in sorted(campos):\n",
    "        resultados = dic.buscar_por_campo_semantico(campo)\n",
    "        resultados = [normalizar_campos(e) for e in resultados]\n",
    "        cnt = len(resultados)\n",
    "        print(f\"{campo}: {cnt}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Validar variantes dialectales\n",
    "def validar_variantes():\n",
    "    print(\"VARIANTES DIALECTALES\")\n",
    "    lemas = [\n",
    "        \"achalay\", \"makichay\", \"intuq\", \"kachapuri\", \"achuqalla\", \"Eqop\", \"Pachar\",\n",
    "        \"achacha\", \"achachilla\", \"achala\", \"achallqo\", \"achikyay\", \"anka\", \"antipurutu\"\n",
    "    ]\n",
    "    for lema in lemas:\n",
    "        variantes = dic.obtener_variantes_dialectales(lema)\n",
    "        print(f\"{lema}: {len(variantes)} variantes\")\n",
    "        if variantes:\n",
    "            for v in variantes:\n",
    "                print(f\"  {v}\")\n",
    "        else:\n",
    "            print(\"  (sin variantes)\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def validar_sinonimos():\n",
    "    print(\"SINÓNIMOS\")\n",
    "    lemas = [\n",
    "        \"achalay\", \"makichay\", \"intuq\", \"kachapuri\", \"achuqalla\", \"Eqop\", \"Pachar\",\n",
    "        \"achacha\", \"achachilla\", \"achala\", \"achallqo\", \"achikyay\", \"anka\", \"antipurutu\"\n",
    "    ]\n",
    "    for lema in lemas:\n",
    "        res = dic.buscar_por_quechua(lema)\n",
    "        res = [normalizar_campos(e) for e in res]\n",
    "        sinonimos = []\n",
    "        for entrada in res:\n",
    "            sins = entrada.get('sinonimos', [])\n",
    "            sinonimos.extend(sins)\n",
    "        if sinonimos:\n",
    "            print(f\"{lema}: {', '.join(sinonimos)}\")\n",
    "        else:\n",
    "            print(f\"{lema}: (no tiene)\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        validar_conteos()\n",
    "        probar_busquedas()\n",
    "        validar_categorias() \n",
    "        validar_campos()\n",
    "        validar_variantes()\n",
    "        validar_sinonimos()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc135edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACIÓN DE CONTEOS DE ENTRADA\n",
      "Quechua-Español\n",
      "  Entradas manuales en JSON: 15039\n",
      "  Entradas reportadas por la librería: 15039\n",
      "  Coincidencia: Sí\n",
      "\n",
      "Español-Quechua\n",
      "  Entradas manuales en JSON: 5526\n",
      "  Entradas reportadas por la librería: 5526\n",
      "  Coincidencia: Sí\n",
      "\n",
      "Los conteos coinciden correctamente.\n",
      "\n",
      "PRUEBAS DE BÚSQUEDA DE LEMAS\n",
      "Lema 'achalay' (Quechua): 1 resultado(s). Definición: Ataviar, adornar, acicalar\n",
      "Lema 'makichay' (Quechua): 1 resultado(s). Definición: Poner las manos o brazos a alguien o algo, en general\n",
      "Lema 'intuq' (Quechua): 1 resultado(s). Definición: y s. Cercador, sitiador, bloqueador\n",
      "Lema 'kachapuri' (Quechua): 1 resultado(s). Definición: Enviado, ordenanza, mandadero. Rufián, alcahuete, tercero\n",
      "Lema 'achuqalla' (Quechua): 1 resultado(s). Definición: (Mustela frenata Lich.) Comadreja. Mamífero mustélido, semip\n",
      "Lema 'Eqop' (Quechua): 1 resultado(s). Definición: Importante mina de plata en el distrito de Carhuaz, provinci\n",
      "Lema 'Pachar' (Quechua): 1 resultado(s). Definición: Conjunto arqueológico de regular tamaño, ubicado en el lugar\n",
      "Lema 'viajar' (Español): 1 resultado(s). Definición: Riy\n",
      "Lema 'cachorro' (Español): 1 resultado(s). Definición: T'ini\n",
      "Lema 'ardid' (Español): 1 resultado(s). Definición: Qaytu\n",
      "Lema 'entrevista' (Español): 1 resultado(s). Definición: Tupay, tinkuy, rikunakuy\n",
      "Lema 'zurcir' (Español): 1 resultado(s). Definición: T'iriy, qeqoy\n",
      "Lemas encontrados: 12 de 12\n",
      "\n",
      "CATEGORÍAS GRAMATICALES PARA QECHUA-ESPAÑOL\n",
      "Total categorías: 7\n",
      "adj.: 4053\n",
      "adv.: 369\n",
      "conj.: 9\n",
      "interj.: 92\n",
      "pron.: 24\n",
      "s.: 5431\n",
      "v.: 5663\n",
      "\n",
      "CAMPOS SEMÁNTICOS\n",
      "Total campos: 54\n",
      "Agri.: 286\n",
      "Anat.: 159\n",
      "Antrop.: 7\n",
      "Apic.: 2\n",
      "Arq.: 6\n",
      "Arqueol.: 158\n",
      "Astrol.: 2\n",
      "Astron.: 26\n",
      "Biol.: 2\n",
      "Bot.: 426\n",
      "Clim.: 56\n",
      "Ecol.Veg.: 26\n",
      "Econ.: 2\n",
      "Educ.: 1\n",
      "Escul.: 2\n",
      "Etnohist.: 193\n",
      "Filos.: 7\n",
      "Fisiol.: 74\n",
      "Fitogeog.: 1\n",
      "Folk.: 177\n",
      "Fís.: 2\n",
      "Geog.: 312\n",
      "Geol.: 33\n",
      "Geom.: 16\n",
      "Gram.: 28\n",
      "Hist.: 261\n",
      "Ingen.: 2\n",
      "Joy.: 5\n",
      "Juris.: 99\n",
      "Ling.: 7\n",
      "Lit.: 21\n",
      "Lit.Ink.: 1\n",
      "Mar.: 2\n",
      "Mat.: 3\n",
      "Med.: 132\n",
      "Med.Folk.: 107\n",
      "Metal.: 12\n",
      "Meteor.: 31\n",
      "Min.: 10\n",
      "Miner.: 22\n",
      "Mit.: 15\n",
      "Mús.: 80\n",
      "Obst.: 6\n",
      "Orfeb.: 2\n",
      "Pat.: 162\n",
      "Psic.: 4\n",
      "Quím.: 7\n",
      "Relig.: 17\n",
      "S.: 7\n",
      "Sociol.: 1\n",
      "Topón.: 1\n",
      "Veter.: 3\n",
      "Zool.: 288\n",
      "Zoot.: 21\n",
      "\n",
      "VARIANTES DIALECTALES\n",
      "achalay: 0 variantes\n",
      "  (sin variantes)\n",
      "makichay: 0 variantes\n",
      "  (sin variantes)\n",
      "intuq: 0 variantes\n",
      "  (sin variantes)\n",
      "kachapuri: 0 variantes\n",
      "  (sin variantes)\n",
      "achuqalla: 5 variantes\n",
      "  Pe/Anc: waywash, mashallu\n",
      "  Pe/Caj: waywash, mashallu\n",
      "  Pe/Aya: achoqlla, chukuri paku, chunpullo\n",
      "  Pe/Jun: unchuchukuy\n",
      "  Ec: achuklla\n",
      "Eqop: 0 variantes\n",
      "  (sin variantes)\n",
      "Pachar: 0 variantes\n",
      "  (sin variantes)\n",
      "achacha: 5 variantes\n",
      "  Pe/Aya: pujllana\n",
      "  Pe/Anc: pujllana\n",
      "  Pe/Caj: pujllana\n",
      "  Arg: achala, achocha\n",
      "  Bol: pukllana, phukllana\n",
      "achachilla: 1 variantes\n",
      "  Ec: Veneración de los accidentes geográficos, considerados como lugares sagrados\n",
      "achala: 1 variantes\n",
      "  Arg: Juguete de niños\n",
      "achallqo: 3 variantes\n",
      "  Pe/Aya: achalku\n",
      "  Bol: achallqo, phumi\n",
      "  Ec: akchallo\n",
      "achikyay: 2 variantes\n",
      "  Pe/Apu: achij (luz, claridad, resplandor)\n",
      "  Pe/Aya: achij (luz, claridad, resplandor)\n",
      "anka: 4 variantes\n",
      "  Pe/Anc: rukus\n",
      "  Pe/Jun: rukus\n",
      "  Pe/Aya: anca (águila real andina)\n",
      "  Arg: anca\n",
      "antipurutu: 0 variantes\n",
      "  (sin variantes)\n",
      "\n",
      "SINÓNIMOS\n",
      "achalay: (no tiene)\n",
      "makichay: (no tiene)\n",
      "intuq: (no tiene)\n",
      "kachapuri: (no tiene)\n",
      "achuqalla: qatay, qataycha\n",
      "Eqop: (no tiene)\n",
      "Pachar: (no tiene)\n",
      "achacha: pukllana\n",
      "achachilla: apachita\n",
      "achala: (no tiene)\n",
      "achallqo: sara chukcha\n",
      "achikyay: (no tiene)\n",
      "anka: (no tiene)\n",
      "antipurutu: (no tiene)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar validaciones y pruebas\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
