{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f7bcf4",
   "metadata": {},
   "source": [
    "# CONSTRUCCIÓN DE RECURSOS LÉXICOS PARA QUECHUA\n",
    "\n",
    "## Objetivo\n",
    "Extraer, estructurar y enriquecer el diccionario bilingüe Quechua-Español para crear un corpus léxico machine-readable.\n",
    "\n",
    "## Estructura del Proyecto\n",
    "- **Entrada**: Diccionario PDF bilingüe (Quechua-Español / Español-Quechua)\n",
    "- **Salida**: Archivos JSON estructurados + librería Python para consultas\n",
    "\n",
    "## Tareas\n",
    "1. Extracción de texto crudo del PDF\n",
    "2. Identificación y separación de secciones\n",
    "3. Diseño e implementación de parsers\n",
    "4. Generación de archivos JSON estructurados\n",
    "5. Desarrollo de librería de utilidades\n",
    "6. Validación y testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dfd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías instaladas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías necesarias\n",
    "# !pip install PyMuPDF pdfplumber regex\n",
    "\n",
    "# Importaciones\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Librerías instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940bb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo texto del PDF...\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extraído: 1930001 caracteres\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extraído: 1930001 caracteres\n"
     ]
    }
   ],
   "source": [
    "# TAREA 1 y 2: Extracción de texto del PDF y separación de secciones\n",
    "\n",
    "def extraer_texto_pdf(ruta_pdf: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae texto crudo del PDF preservando estructura básica sin alteraciones\n",
    "    \"\"\"\n",
    "    texto_completo = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Usar PyMuPDF para extracción\n",
    "        doc = fitz.open(ruta_pdf)\n",
    "        \n",
    "        for pagina_num in range(doc.page_count):\n",
    "            pagina = doc[pagina_num]\n",
    "            texto = pagina.get_text()\n",
    "            texto_completo += texto\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error con PyMuPDF: {e}\")\n",
    "        \n",
    "        # Alternativa con pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(ruta_pdf) as pdf:\n",
    "                for pagina in pdf.pages:\n",
    "                    texto = pagina.extract_text()\n",
    "                    if texto:\n",
    "                        texto_completo += texto\n",
    "        except Exception as e2:\n",
    "            print(f\"Error con pdfplumber: {e2}\")\n",
    "            return \"\"\n",
    "    \n",
    "    return texto_completo\n",
    "\n",
    "def guardar_texto_crudo(texto: str, ruta_salida: str = \"diccionario_raw.txt\"):\n",
    "    \"\"\"\n",
    "    Guarda el texto crudo extraído del PDF\n",
    "    \"\"\"\n",
    "    with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "        f.write(texto)\n",
    "    print(f\"Texto crudo guardado en: {ruta_salida}\")\n",
    "\n",
    "# Ejecutar extracción\n",
    "ruta_pdf = \"diccionario-qeswa-academia-mayor.pdf\"\n",
    "if os.path.exists(ruta_pdf):\n",
    "    print(\"Extrayendo texto del PDF...\")\n",
    "    texto_crudo = extraer_texto_pdf(ruta_pdf)\n",
    "    guardar_texto_crudo(texto_crudo)\n",
    "    print(f\"Texto extraído: {len(texto_crudo)} caracteres\")\n",
    "else:\n",
    "    print(f\"Archivo PDF no encontrado: {ruta_pdf}\")\n",
    "    print(\"Por favor, asegúrese de que el archivo esté en el directorio correcto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d73b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo abreviaturas...\n",
      "Dialectales: 18\n",
      "Categorías gramaticales: 46\n",
      "Campos semánticos: 93\n",
      "\n",
      "Separando secciones...\n",
      "Sección Quechua-Español: 1615928 caracteres\n",
      "Sección Español-Quechua: 277475 caracteres\n"
     ]
    }
   ],
   "source": [
    "# TAREA 3: Identificación de secciones y extracción automática de abreviaturas\n",
    "\n",
    "def extraer_abreviaturas_automaticamente(texto: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extrae automáticamente abreviaturas dialectales, categorías gramaticales y campos semánticos\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    \n",
    "    abreviaturas = {\n",
    "        'categorias_gramaticales': [],\n",
    "        'campos_semanticos': [],\n",
    "        'dialectales': []\n",
    "    }\n",
    "    \n",
    "    # 1. EXTRAER ABREVIATURAS DIALECTALES\n",
    "    dialectales_extraidas = set()\n",
    "    \n",
    "    for i in range(792, min(853, len(lineas))):\n",
    "        linea = lineas[i].strip()\n",
    "        \n",
    "        # Países: \"1. Arg.\"\n",
    "        match_pais = re.match(r'^\\d+\\.\\s+([A-Z][a-z]*\\.)\\s*$', linea)\n",
    "        if match_pais:\n",
    "            dialectales_extraidas.add(match_pais.group(1))\n",
    "        \n",
    "        # Regiones peruanas: \"1. Pe.Anc.\"\n",
    "        match_region = re.match(r'^\\d+\\.\\s+(Pe\\.[A-Za-z]+\\.)\\s*$', linea)\n",
    "        if match_region:\n",
    "            dialectales_extraidas.add(match_region.group(1))\n",
    "    \n",
    "    abreviaturas['dialectales'] = sorted(list(dialectales_extraidas))\n",
    "    \n",
    "    # 2. EXTRAER ABREVIATURAS DE CATEGORÍAS Y CAMPOS SEMÁNTICOS\n",
    "    inicio_abreviaturas = 853\n",
    "    \n",
    "    # Buscar fin de sección\n",
    "    fin_abreviaturas = 1320  # Default\n",
    "    for i in range(1000, min(1400, len(lineas))):\n",
    "        linea = lineas[i].strip().upper()\n",
    "        if \"AUTORES\" in linea and \"CONSULTADOS\" in linea:\n",
    "            fin_abreviaturas = i\n",
    "            break\n",
    "    \n",
    "    # Extraer abreviaturas\n",
    "    todas_abreviaturas = set()\n",
    "    \n",
    "    for i in range(inicio_abreviaturas, fin_abreviaturas):\n",
    "        if i < len(lineas):\n",
    "            linea = lineas[i].strip()\n",
    "            \n",
    "            if (linea and linea.endswith('.') and len(linea) <= 25 and \n",
    "                ' ' not in linea and len(linea) >= 2 and\n",
    "                not re.match(r'^\\d+\\.?$', linea) and \n",
    "                not re.match(r'^[^a-zA-Z]+\\.$', linea)):\n",
    "                todas_abreviaturas.add(linea)\n",
    "    \n",
    "    # 3. CLASIFICAR ABREVIATURAS\n",
    "    categorias_gramaticales = [\n",
    "        'adj.', 'adv.', 's.', 'v.', 'interj.', 'prep.', 'conj.', 'pron.',\n",
    "        'm.', 'f.', 'pl.', 'sing.', 'loc.', 'loc.adv.', 'núm.', 'núm.card.',\n",
    "        'núm.ord.', 'imper.', 'infínit.', 'interrog.', 'negat.', 'gen.',\n",
    "        'alfab.', 'diminut.', 'antón.', 'sinón.', 'parón.', 'apóc.',\n",
    "        'etim.', 'onomat.', 'neol.', 'figdo.', 'fam.',\n",
    "        'ejem.', 'dep.', 'dist.', 'prov.', 'bibliogr.', 'calend.',\n",
    "        'comer.', 'medid.', 'pref.', 'suf.', 'tej.', 'S.', 'alim.'\n",
    "    ]\n",
    "    \n",
    "    categorias_encontradas = set()\n",
    "    campos_semanticos = set()\n",
    "    \n",
    "    for abrev in todas_abreviaturas:\n",
    "        if abrev in categorias_gramaticales:\n",
    "            categorias_encontradas.add(abrev)\n",
    "        else:\n",
    "            campos_semanticos.add(abrev)\n",
    "    \n",
    "    abreviaturas['categorias_gramaticales'] = sorted(list(categorias_encontradas))\n",
    "    abreviaturas['campos_semanticos'] = sorted(list(campos_semanticos))\n",
    "    \n",
    "    return abreviaturas\n",
    "\n",
    "def separar_secciones_por_lineas(texto: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Separa las secciones usando los números de línea conocidos\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    \n",
    "    # Sección Quechua-Español: línea 1325 a 50998\n",
    "    seccion_qe_lineas = lineas[1325:50998]\n",
    "    \n",
    "    # Limpiar texto extra al final de la sección Quechua-Español\n",
    "    seccion_qe_limpia = []\n",
    "    for linea in seccion_qe_lineas:\n",
    "        if \"ESPAÑOL - QUECHUA\" in linea:\n",
    "            break\n",
    "        seccion_qe_limpia.append(linea)\n",
    "    \n",
    "    # Sección Español-Quechua: línea 50998 hasta el final\n",
    "    seccion_eq_lineas = []\n",
    "    for i in range(50998, len(lineas)):\n",
    "        linea = lineas[i].strip()\n",
    "        if (\"DICCIONARIO QUECHUA – ESPAÑOL – QUECHUA\" in linea or \n",
    "            \"se terminó de imprimir\" in linea or \n",
    "            \"talleres gráficos\" in linea):\n",
    "            break\n",
    "        seccion_eq_lineas.append(lineas[i])\n",
    "    \n",
    "    return '\\n'.join(seccion_qe_limpia), '\\n'.join(seccion_eq_lineas)\n",
    "\n",
    "# Ejecutar extracción\n",
    "if os.path.exists(\"diccionario_raw.txt\"):\n",
    "    with open(\"diccionario_raw.txt\", 'r', encoding='utf-8') as f:\n",
    "        texto_completo = f.read()\n",
    "    \n",
    "    print(\"Extrayendo abreviaturas...\")\n",
    "    abreviaturas = extraer_abreviaturas_automaticamente(texto_completo)\n",
    "    \n",
    "    print(f\"Dialectales: {len(abreviaturas['dialectales'])}\")\n",
    "    print(f\"Categorías gramaticales: {len(abreviaturas['categorias_gramaticales'])}\")\n",
    "    print(f\"Campos semánticos: {len(abreviaturas['campos_semanticos'])}\")\n",
    "    \n",
    "    print(\"\\nSeparando secciones...\")\n",
    "    seccion_qe, seccion_eq = separar_secciones_por_lineas(texto_completo)\n",
    "    \n",
    "    print(f\"Sección Quechua-Español: {len(seccion_qe)} caracteres\")\n",
    "    print(f\"Sección Español-Quechua: {len(seccion_eq)} caracteres\")\n",
    "    \n",
    "    # Guardar archivos\n",
    "    with open(\"seccion_quechua_espanol.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_qe)\n",
    "    \n",
    "    with open(\"seccion_espanol_quechua.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_eq)\n",
    "    \n",
    "    with open(\"abreviaturas.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(abreviaturas, f, indent=2, ensure_ascii=False)\n",
    "else:\n",
    "    print(\"Archivo diccionario_raw.txt no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcdafa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsers ultra-estrictos implementados correctamente\n",
      "Parsers ultra-estrictos cargados con correcciones según las 4 REGLAS\n",
      "=== PRUEBAS DE CASOS ESPECÍFICOS ===\n",
      "\n",
      "1. CASO aa! (variante dialectal después de ||):\n",
      "   Definición: ¡Oh!, ¡ah! Arcaísmo de a\n",
      "   Variantes: {'Arg': ['Fuera', 'afuera']}\n",
      "   ✅ DETECTÓ variante dialectal\n",
      "\n",
      "2. CASO achiku (acepción + variante después de ||):\n",
      "   Definición: Estrafalario. Gracioso\n",
      "   Variantes: {'Arg': ['achika']}\n",
      "   ✅ DETECTÓ acepción Y variante\n",
      "\n",
      "3. CASO achikyay (variante dialectal compleja después de ||):\n",
      "   Definición: Rayar la aurora. Centellear, titilar las primeras luces del amanecer\n",
      "   Variantes: {'Pe': {'Apu': ['achij (luz', 'claridad', 'resplandor)'], 'Aya': ['achij (luz', 'claridad', 'resplandor)']}}\n",
      "   ✅ DETECTÓ variante dialectal compleja\n"
     ]
    }
   ],
   "source": [
    "# TAREA 4: Diseño e implementación de parsers\n",
    "\n",
    "def extraer_entradas_completas(texto: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrae entradas completas del texto, incluyendo las que abarcan múltiples líneas.\n",
    "    Mejorado para manejar lemas compuestos (ej: \"Amaru Inka Yupanki\") y apostrofes (ej: \"map'a\")\n",
    "    CORRIGE: Problema de absorción de lemas siguientes con remisiones V.\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    entradas = []\n",
    "    entrada_actual = \"\"\n",
    "    \n",
    "    # Patrón mejorado que incluye apostrofes y detecta inicios de entrada\n",
    "    patron_inicio = r'^[a-zA-ZÀ-ÿñÑ!¡¿?,\\s\\']+[\\.!]\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.)'\n",
    "    \n",
    "    for linea in lineas:\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Verificar si es el inicio de una nueva entrada\n",
    "        if re.match(patron_inicio, linea):\n",
    "            # Guardar entrada anterior si existe\n",
    "            if entrada_actual.strip():\n",
    "                entradas.append(entrada_actual.strip())\n",
    "            # Iniciar nueva entrada\n",
    "            entrada_actual = linea\n",
    "        else:\n",
    "            # Continuar entrada actual\n",
    "            if entrada_actual:\n",
    "                entrada_actual += \" \" + linea\n",
    "    \n",
    "    # Agregar la última entrada\n",
    "    if entrada_actual.strip():\n",
    "        entradas.append(entrada_actual.strip())\n",
    "    \n",
    "    # POST-PROCESAMIENTO: Separar entradas que fueron incorrectamente unidas\n",
    "    entradas_corregidas = []\n",
    "    \n",
    "    for entrada in entradas:\n",
    "        # Buscar patrones de lemas que fueron absorbidos incorrectamente\n",
    "        # Patrón: texto completo + lema. categoria V. REFERENCIA\n",
    "        patron_absorcion = r'^(.+?)\\s+([a-zA-ZÀ-ÿñÑ!¡¿?,\\s\\']+[\\.!])\\s+(V\\.\\s+[A-ZÁÉÍÓÚÑ\\s]+\\.?)\\s*$'\n",
    "        match = re.match(patron_absorcion, entrada)\n",
    "        \n",
    "        if match:\n",
    "            # Separar las entradas\n",
    "            entrada_principal = match.group(1).strip()\n",
    "            lema_absorbido = match.group(2).strip()\n",
    "            remision = match.group(3).strip()\n",
    "            \n",
    "            # Verificar que el lema absorbido es realmente un lema válido\n",
    "            if re.match(r'^[a-zA-ZÀ-ÿñÑ!¡¿?,\\s\\']+[\\.!]$', lema_absorbido):\n",
    "                entradas_corregidas.append(entrada_principal)\n",
    "                entradas_corregidas.append(f\"{lema_absorbido} {remision}\")\n",
    "            else:\n",
    "                entradas_corregidas.append(entrada)\n",
    "        else:\n",
    "            entradas_corregidas.append(entrada)\n",
    "    \n",
    "    return entradas_corregidas\n",
    "\n",
    "def parsear_entrada_ultra_estricta(entrada_texto: str, abreviaturas: Dict, es_espanol: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parser ULTRA-ESTRICTO mejorado según todas las especificaciones del usuario.\n",
    "    Maneja: lemas compuestos, remisiones (V.), definiciones múltiples, || como separador de acepciones/dialectales.\n",
    "    \"\"\"\n",
    "    if len(entrada_texto) < 10:\n",
    "        return []\n",
    "    \n",
    "    # 1. EXTRAER LEMA (puede ser compuesto: \"Amaru Inka Yupanki\")\n",
    "    # Patrón mejorado para lemas compuestos\n",
    "    match_lema = re.match(r'^([a-zA-ZÀ-ÿñÑ!¡¿?,\\s]+?)[\\.!]\\s', entrada_texto)\n",
    "    if not match_lema:\n",
    "        return []\n",
    "    \n",
    "    lema = match_lema.group(1).strip()\n",
    "    # Mantener mayúsculas para lemas compuestos (ej: \"Amaru Inka Yupanki\")\n",
    "    # Solo convertir a minúscula si es una sola palabra que no empieza con mayúscula\n",
    "    if ' ' not in lema and not lema[0].isupper():\n",
    "        lema = lema.lower()\n",
    "    \n",
    "    # Extraer resto del texto después del lema\n",
    "    resto = entrada_texto[len(match_lema.group(0)):].strip()\n",
    "    \n",
    "    # Filtro para español: evitar palabras quechuas\n",
    "    if es_espanol:\n",
    "        if any(c in lema.lower() for c in ['k', 'w', 'q']) and 'qu' not in lema.lower():\n",
    "            return []\n",
    "    \n",
    "    # Obtener listas de abreviaturas\n",
    "    categorias_gramaticales = abreviaturas.get('categorias_gramaticales', [])\n",
    "    campos_semanticos = abreviaturas.get('campos_semanticos', [])\n",
    "    dialectales = abreviaturas.get('dialectales', [])\n",
    "    \n",
    "    # 2. EXTRAER CATEGORÍA GRAMATICAL (inmediatamente después del lema)\n",
    "    categoria = ''\n",
    "    for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "        if resto.startswith(cat):\n",
    "            categoria = cat\n",
    "            resto = resto[len(cat):].strip()\n",
    "            break\n",
    "    \n",
    "    if not categoria:\n",
    "        return []\n",
    "    \n",
    "    # 3. DETECTAR TODOS LOS CAMPOS SEMÁNTICOS EN TODA LA DEFINICIÓN (REGLA 1)\n",
    "    campos_encontrados = []\n",
    "    texto_busqueda = resto\n",
    "    \n",
    "    # Buscar TODOS los campos semánticos en todo el texto, no solo al inicio\n",
    "    for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "        # Buscar en diferentes posiciones: inicio, después de punto, en medio\n",
    "        patrones_busqueda = [\n",
    "            f'^{re.escape(campo)}\\\\s',          # Al inicio\n",
    "            f'\\\\s{re.escape(campo)}\\\\s',        # En medio del texto\n",
    "            f'\\\\.\\\\s*{re.escape(campo)}\\\\s',    # Después de punto\n",
    "            f'^{re.escape(campo)}\\\\.',          # Al inicio con punto\n",
    "            f'\\\\s{re.escape(campo)}\\\\.'         # En medio con punto\n",
    "        ]\n",
    "        \n",
    "        for patron in patrones_busqueda:\n",
    "            if re.search(patron, texto_busqueda):\n",
    "                if campo not in campos_encontrados:\n",
    "                    campos_encontrados.append(campo)\n",
    "                break\n",
    "    \n",
    "    # 4. PROCESAMIENTO MEJORADO DE CONTENIDO DESPUÉS DE ||\n",
    "    if '||' in resto:\n",
    "        partes_separadas = [a.strip() for a in resto.split('||') if a.strip()]\n",
    "    else:\n",
    "        partes_separadas = [resto]\n",
    "    \n",
    "    # Estructura única de entrada que iremos completando\n",
    "    entrada = {\n",
    "        'lema': lema,\n",
    "        'categoria_gramatical': [categoria],  # Lista para múltiples categorías\n",
    "        'campo_semantico': campos_encontrados.copy(),\n",
    "        'definicion': '',\n",
    "        'variantes_dialectales': {},\n",
    "        'sinonimos': [],\n",
    "        'ejemplos': []\n",
    "    }\n",
    "    \n",
    "    # Lista para acumular todas las definiciones\n",
    "    todas_definiciones = []\n",
    "    \n",
    "    for i, parte in enumerate(partes_separadas):\n",
    "        texto = parte.strip()\n",
    "        \n",
    "        # 5. DETECTAR REMISIONES \"V. LEMA\" (véase)\n",
    "        match_remision = re.search(r'\\bV\\.\\s+([A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*)', texto)\n",
    "        if match_remision:\n",
    "            remite_a = match_remision.group(1).strip().lower()\n",
    "            entrada['remite_a'] = remite_a\n",
    "            texto = re.sub(r'\\bV\\.\\s+[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*', '', texto).strip()\n",
    "        \n",
    "        # 6. DETECTAR CATEGORÍA GRAMATICAL ADICIONAL (para casos como \"|| adj. insult.\")\n",
    "        categoria_adicional = ''\n",
    "        for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "            if texto.startswith(cat):\n",
    "                categoria_adicional = cat\n",
    "                if cat not in entrada['categoria_gramatical']:\n",
    "                    entrada['categoria_gramatical'].append(cat)\n",
    "                texto = texto[len(cat):].strip()\n",
    "                break\n",
    "        \n",
    "        # 7. DETECTAR CAMPOS SEMÁNTICOS ADICIONALES (para casos como \"|| Agri.\")\n",
    "        campos_adicionales = []\n",
    "        for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "            if texto.startswith(campo):\n",
    "                campos_adicionales.append(campo)\n",
    "                if campo not in entrada['campo_semantico']:\n",
    "                    entrada['campo_semantico'].append(campo)\n",
    "                texto = texto[len(campo):].strip()\n",
    "                break\n",
    "        \n",
    "        # 8. DETERMINAR TIPO DE CONTENIDO DESPUÉS DE ||\n",
    "        es_dialectal = False\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            if (f'{dialecto_sin_punto}:' in texto or \n",
    "                texto.startswith(dialecto_sin_punto + ':') or\n",
    "                f' {dialecto_sin_punto}:' in texto):\n",
    "                es_dialectal = True\n",
    "                break\n",
    "        \n",
    "        # 9. EXTRAER EJEMPLOS (EJEM:)\n",
    "        if 'EJEM:' in texto:\n",
    "            pos_ejem = texto.find('EJEM:')\n",
    "            texto_antes = texto[:pos_ejem].strip()\n",
    "            texto_ejem = texto[pos_ejem + 5:].strip()\n",
    "            \n",
    "            # Encontrar final del ejemplo (hasta dialecto o final)\n",
    "            fin_ejemplo = len(texto_ejem)\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                pos_dialecto = texto_ejem.find(f'{dialecto_sin_punto}:')\n",
    "                if pos_dialecto != -1 and pos_dialecto < fin_ejemplo:\n",
    "                    fin_ejemplo = pos_dialecto\n",
    "            \n",
    "            ejemplo_texto = texto_ejem[:fin_ejemplo].strip(' .,!')\n",
    "            if ejemplo_texto and ejemplo_texto not in entrada['ejemplos']:\n",
    "                entrada['ejemplos'].append(ejemplo_texto)\n",
    "            \n",
    "            # Continuar con el resto después del ejemplo\n",
    "            resto_ejemplo = texto_ejem[fin_ejemplo:].strip()\n",
    "            texto = texto_antes + ' ' + resto_ejemplo\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 8. EXTRAER SINÓNIMOS (SINÓN:) \n",
    "        if 'SINÓN:' in texto:\n",
    "            pos_sinon = texto.find('SINÓN:')\n",
    "            texto_antes = texto[:pos_sinon].strip()\n",
    "            texto_sinon = texto[pos_sinon + 6:].strip()\n",
    "            \n",
    "            # Los sinónimos terminan en el PRIMER punto seguido de mayúscula O dialecto\n",
    "            fin_sinon = len(texto_sinon)\n",
    "            \n",
    "            # Buscar primer punto seguido de mayúscula\n",
    "            for j, char in enumerate(texto_sinon):\n",
    "                if char == '.' and j + 1 < len(texto_sinon):\n",
    "                    siguiente = texto_sinon[j + 1:j + 10].strip()\n",
    "                    if siguiente and siguiente[0].isupper():\n",
    "                        fin_sinon = j + 1\n",
    "                        break\n",
    "            \n",
    "            # Buscar abreviaturas dialectales que interrumpen sinónimos\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                for patron in [f' {dialecto_sin_punto}:', f' {dialecto_sin_punto} ']:\n",
    "                    pos = texto_sinon.find(patron)\n",
    "                    if pos != -1 and pos < fin_sinon:\n",
    "                        fin_sinon = pos\n",
    "            \n",
    "            sinon_texto = texto_sinon[:fin_sinon].strip(' .,!')\n",
    "            if sinon_texto:\n",
    "                sinonimos = [s.strip() for s in sinon_texto.split(',') if s.strip()]\n",
    "                entrada['sinonimos'] = [re.sub(r'[^\\w\\sñÑáéíóúÁÉÍÓÚ]', '', s).strip() \n",
    "                                      for s in sinonimos if len(s.strip()) > 1]\n",
    "            \n",
    "            # Continuar con el resto después de los sinónimos\n",
    "            resto_despues_sinon = texto_sinon[fin_sinon:].strip()\n",
    "            texto = texto_antes + ' ' + resto_despues_sinon\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 11. EXTRAER VARIANTES DIALECTALES (MEJORADO PARA ||)\n",
    "        texto_limpio = texto\n",
    "        dialectales_ordenados = sorted(dialectales, key=len, reverse=True)\n",
    "        \n",
    "        for dialecto in dialectales_ordenados:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            \n",
    "            # Buscar patrón más flexible: \"Arg:\", \" Arg:\", al inicio o después de espacios\n",
    "            patrones_dialectal = [\n",
    "                f'{dialecto_sin_punto}:',      # Al inicio: \"Arg:\"\n",
    "                f' {dialecto_sin_punto}:',     # En medio: \" Arg:\"\n",
    "                f'^{dialecto_sin_punto}:'      # Exactamente al inicio\n",
    "            ]\n",
    "            \n",
    "            pos_encontrada = -1\n",
    "            patron_usado = ''\n",
    "            \n",
    "            for patron in patrones_dialectal:\n",
    "                if patron.startswith('^'):\n",
    "                    # Patrón de inicio exacto\n",
    "                    if texto_limpio.startswith(patron[1:]):\n",
    "                        pos_encontrada = 0\n",
    "                        patron_usado = patron[1:]\n",
    "                        break\n",
    "                else:\n",
    "                    pos = texto_limpio.find(patron)\n",
    "                    if pos != -1:\n",
    "                        pos_encontrada = pos\n",
    "                        patron_usado = patron\n",
    "                        break\n",
    "            \n",
    "            if pos_encontrada != -1:\n",
    "                texto_antes = texto_limpio[:pos_encontrada].strip()\n",
    "                texto_despues = texto_limpio[pos_encontrada + len(patron_usado):].strip()\n",
    "                \n",
    "                # Encontrar final de variantes dialectales\n",
    "                fin_dialectal = len(texto_despues)\n",
    "                \n",
    "                # Buscar próximo dialecto\n",
    "                for otro_dialecto in dialectales_ordenados:\n",
    "                    if otro_dialecto != dialecto:\n",
    "                        otro_sin_punto = otro_dialecto.rstrip('.')\n",
    "                        for patron_otro in [f'{otro_sin_punto}:', f' {otro_sin_punto}:']:\n",
    "                            pos_otro = texto_despues.find(patron_otro)\n",
    "                            if pos_otro != -1 and pos_otro < fin_dialectal:\n",
    "                                fin_dialectal = pos_otro\n",
    "                \n",
    "                # Buscar marcadores que interrumpen\n",
    "                for marcador in [' SINÓN:', ' EJEM:', '. SINÓN:', '. EJEM:']:\n",
    "                    pos_marcador = texto_despues.find(marcador)\n",
    "                    if pos_marcador != -1 and pos_marcador < fin_dialectal:\n",
    "                        fin_dialectal = pos_marcador\n",
    "                \n",
    "                contenido_dialectal = texto_despues[:fin_dialectal].strip(' .,!')\n",
    "                \n",
    "                if contenido_dialectal:\n",
    "                    # ANALIZAR CONTENIDO: formato \"Anc: Caj: pujllana\" o \"achala, achocha\"\n",
    "                    regiones_adicionales = []\n",
    "                    variantes_finales = []\n",
    "                    \n",
    "                    # Separar por dos puntos para detectar regiones comprimidas\n",
    "                    partes_dos_puntos = contenido_dialectal.split(':')\n",
    "                    \n",
    "                    if len(partes_dos_puntos) > 1:\n",
    "                        # Hay formato comprimido: \"Anc: Caj: pujllana\"\n",
    "                        for parte in partes_dos_puntos[:-1]:  # Todas excepto la última\n",
    "                            parte = parte.strip()\n",
    "                            # Verificar si es una región válida (3-4 letras, empieza con mayúscula)\n",
    "                            if re.match(r'^[A-Z][a-z]{2,3}$', parte):\n",
    "                                regiones_adicionales.append(parte)\n",
    "                        \n",
    "                        # La última parte son las variantes\n",
    "                        variantes_texto = partes_dos_puntos[-1].strip()\n",
    "                        variantes_finales = [v.strip() for v in variantes_texto.split(',') if v.strip()]\n",
    "                    else:\n",
    "                        # Formato simple: \"achala, achocha\"\n",
    "                        variantes_finales = [v.strip() for v in contenido_dialectal.split(',') if v.strip()]\n",
    "                    \n",
    "                    # CREAR ESTRUCTURA DE VARIANTES DIALECTALES (SIN \"Gen\")\n",
    "                    if '.' in dialecto and len(dialecto.split('.', 1)[1].rstrip('.')) > 0:\n",
    "                        # Caso: Pe.Aya. → pais='Pe', region='Aya'\n",
    "                        pais, region_principal = dialecto.split('.', 1)\n",
    "                        region_principal = region_principal.rstrip('.')\n",
    "                        \n",
    "                        if pais not in entrada['variantes_dialectales']:\n",
    "                            entrada['variantes_dialectales'][pais] = {}\n",
    "                        \n",
    "                        # Agregar región principal\n",
    "                        entrada['variantes_dialectales'][pais][region_principal] = variantes_finales\n",
    "                        \n",
    "                        # Agregar regiones adicionales con las mismas variantes\n",
    "                        for region_adicional in regiones_adicionales:\n",
    "                            entrada['variantes_dialectales'][pais][region_adicional] = variantes_finales\n",
    "                            \n",
    "                    else:\n",
    "                        # Caso: Arg. → pais='Arg', sin estructura anidada\n",
    "                        pais = dialecto.rstrip('.')\n",
    "                        entrada['variantes_dialectales'][pais] = variantes_finales\n",
    "                \n",
    "                # Limpiar del texto principal\n",
    "                texto_limpio = texto_antes + ' ' + texto_despues[fin_dialectal:]\n",
    "                texto_limpio = texto_limpio.strip()\n",
    "        \n",
    "        # 10. EXTRAER DEFINICIÓN (lo que queda después de limpiar todo)\n",
    "        definicion = texto_limpio\n",
    "        \n",
    "        # REGLA 1: Limpiar TODOS los campos semánticos detectados de la definición\n",
    "        for campo in campos_encontrados:\n",
    "            # Limpiar campo al inicio de la definición\n",
    "            definicion = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion)\n",
    "            # Limpiar campo en medio de la definición\n",
    "            definicion = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion)\n",
    "            # Limpiar campo después de punto\n",
    "            definicion = re.sub(f'\\\\.\\\\s*{re.escape(campo)}\\\\s*', '. ', definicion)\n",
    "        \n",
    "        # Limpiar campos adicionales encontrados después de ||\n",
    "        for campo in campos_adicionales:\n",
    "            definicion = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion)\n",
    "            definicion = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion)\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion = re.sub(r'SINÓN:.*$', '', definicion)\n",
    "        definicion = re.sub(r'EJEM:.*$', '', definicion)\n",
    "        definicion = re.sub(r'VARIEDADES:.*$', '', definicion, flags=re.IGNORECASE)\n",
    "        definicion = re.sub(r'\\bV\\.\\s+[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*', '', definicion)\n",
    "        \n",
    "        # Limpiar dialectales residuales\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            definicion = re.sub(f'{re.escape(dialecto_sin_punto)}:.*$', '', definicion)\n",
    "        \n",
    "        # 12. EXTRAER DEFINICIÓN (lo que queda después de limpiar todo)\n",
    "        definicion_parte = texto_limpio\n",
    "        \n",
    "        # Limpiar campos semánticos del texto\n",
    "        for campo in entrada['campo_semantico']:\n",
    "            definicion_parte = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion_parte)\n",
    "            definicion_parte = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion_parte)\n",
    "            definicion_parte = re.sub(f'\\\\.\\\\s*{re.escape(campo)}\\\\s*', '. ', definicion_parte)\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion_parte = re.sub(r'SINÓN:.*$', '', definicion_parte)\n",
    "        definicion_parte = re.sub(r'EJEM:.*$', '', definicion_parte)\n",
    "        definicion_parte = re.sub(r'VARIEDADES:.*$', '', definicion_parte, flags=re.IGNORECASE)\n",
    "        definicion_parte = re.sub(r'^\\([^)]*\\)\\.\\s*', '', definicion_parte)  # Nombres científicos solos\n",
    "        \n",
    "        # REGLA 3: Evitar contaminación - detectar nueva entrada\n",
    "        patron_nueva_entrada = r'\\b[a-zA-Z]+\\.\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.|\\bV\\.)'\n",
    "        match_nueva = re.search(patron_nueva_entrada, definicion_parte)\n",
    "        if match_nueva:\n",
    "            definicion_parte = definicion_parte[:match_nueva.start()].strip()\n",
    "        \n",
    "        definicion_parte = re.sub(r'\\s+', ' ', definicion_parte).strip(' .,!')\n",
    "        \n",
    "        # AGREGAR A DEFINICIONES TOTALES\n",
    "        # Primera parte siempre se incluye, partes después de || se incluyen si no son puramente dialectales\n",
    "        if i == 0:\n",
    "            # Primera parte - siempre incluir\n",
    "            if definicion_parte:\n",
    "                todas_definiciones.append(definicion_parte)\n",
    "        else:\n",
    "            # Partes después de || - incluir si hay contenido y no es solo dialectal\n",
    "            if definicion_parte and not es_dialectal:\n",
    "                todas_definiciones.append(definicion_parte)\n",
    "            elif definicion_parte and es_dialectal:\n",
    "                # Si es dialectal pero hay contenido antes del dialecto, incluirlo\n",
    "                texto_antes_dialectos = definicion_parte\n",
    "                for dialecto in dialectales:\n",
    "                    dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                    patron_corte = f'{dialecto_sin_punto}:'\n",
    "                    if patron_corte in texto_antes_dialectos:\n",
    "                        pos_corte = texto_antes_dialectos.find(patron_corte)\n",
    "                        texto_antes_dialectos = texto_antes_dialectos[:pos_corte].strip()\n",
    "                        break\n",
    "                if texto_antes_dialectos:\n",
    "                    todas_definiciones.append(texto_antes_dialectos)\n",
    "        \n",
    "    # COMBINAR TODAS LAS DEFINICIONES\n",
    "    if todas_definiciones:\n",
    "        entrada['definicion'] = '. '.join(todas_definiciones)\n",
    "    \n",
    "    # FORMATEAR CATEGORÍA GRAMATICAL\n",
    "    if len(entrada['categoria_gramatical']) == 1:\n",
    "        entrada['categoria_gramatical'] = entrada['categoria_gramatical'][0]\n",
    "    \n",
    "    # FORMATEAR CAMPO SEMÁNTICO - mantener como lista vacía si no hay campos\n",
    "    if not entrada['campo_semantico']:\n",
    "        entrada['campo_semantico'] = []\n",
    "    \n",
    "    # RETORNAR ENTRADA ÚNICA (ya no necesitamos múltiples entradas por lema)\n",
    "    if (entrada['definicion'] or entrada['sinonimos'] or \n",
    "        entrada['variantes_dialectales'] or entrada.get('remite_a')):\n",
    "        return [entrada]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "class QuechuaEspanolParser:\n",
    "    \"\"\"\n",
    "    Parser específico para la sección Quechua-Español\n",
    "    Utiliza el parser ultra-estricto que separa correctamente los campos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, abreviaturas: Dict[str, List[str]]):\n",
    "        self.abreviaturas = abreviaturas\n",
    "    \n",
    "    def parsear_seccion(self, texto: str, max_entradas: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parsea la sección Quechua-Español con el parser ultra-estricto\n",
    "        \"\"\"\n",
    "        entradas = []\n",
    "        \n",
    "        # Extraer entradas completas (multilinea)\n",
    "        entradas_texto = extraer_entradas_completas(texto)\n",
    "        print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "        \n",
    "        contador = 0\n",
    "        for entrada_texto in entradas_texto:\n",
    "            if contador >= max_entradas:\n",
    "                break\n",
    "                \n",
    "            entradas_procesadas = parsear_entrada_ultra_estricta(entrada_texto, self.abreviaturas, es_espanol=False)\n",
    "            if entradas_procesadas:\n",
    "                entradas.extend(entradas_procesadas)\n",
    "                contador += len(entradas_procesadas)\n",
    "        \n",
    "        return entradas\n",
    "\n",
    "class EspanolQuechuaParser:\n",
    "    \"\"\"\n",
    "    Parser específico para la sección Español-Quechua\n",
    "    Utiliza el parser ultra-estricto que separa correctamente los campos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, abreviaturas: Dict[str, List[str]]):\n",
    "        self.abreviaturas = abreviaturas\n",
    "    \n",
    "    def parsear_seccion(self, texto: str, max_entradas: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parsea la sección Español-Quechua con el parser ultra-estricto\n",
    "        \"\"\"\n",
    "        entradas = []\n",
    "        \n",
    "        # Extraer entradas completas (multilinea)\n",
    "        entradas_texto = extraer_entradas_completas(texto)\n",
    "        print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "        \n",
    "        contador = 0\n",
    "        for entrada_texto in entradas_texto:\n",
    "            if contador >= max_entradas:\n",
    "                break\n",
    "                \n",
    "            entradas_procesadas = parsear_entrada_ultra_estricta(entrada_texto, self.abreviaturas, es_espanol=True)\n",
    "            if entradas_procesadas:\n",
    "                entradas.extend(entradas_procesadas)\n",
    "                contador += len(entradas_procesadas)\n",
    "        \n",
    "        return entradas\n",
    "\n",
    "print(\"✅ Parsers ultra-estrictos implementados correctamente\")\n",
    "print(\"Parsers ultra-estrictos cargados con correcciones según las 4 REGLAS\")\n",
    "\n",
    "# Función de prueba para casos específicos del usuario\n",
    "def probar_casos_usuario():\n",
    "    \"\"\"Prueba casos específicos mencionados por el usuario\"\"\"\n",
    "    # Abreviaturas de prueba\n",
    "    abrev_test = {\n",
    "        'categorias_gramaticales': ['s.', 'adj.', 'v.', 'interj.'],\n",
    "        'campos_semanticos': ['Bot.', 'Med.Folk.', 'Ecol.Veg.', 'Zool.', 'Agri.', 'insult.'],\n",
    "        'dialectales': ['Pe.Aya.', 'Bol.', 'Ec.', 'Arg.', 'Pe.Apu.', 'Pe.Caj.', 'Pe.Jun.']\n",
    "    }\n",
    "    \n",
    "    print(\"=== PRUEBAS DE CASOS ESPECÍFICOS ===\")\n",
    "    \n",
    "    # Caso 1: aa! - variante dialectal después de ||\n",
    "    print(\"\\n1. CASO aa! (variante dialectal después de ||):\")\n",
    "    entrada1 = \"aa! interj. ¡Oh!, ¡ah! Arcaísmo de a! || Arg: Fuera, afuera.\"\n",
    "    resultado1 = parsear_entrada_ultra_estricta(entrada1, abrev_test)\n",
    "    if resultado1:\n",
    "        r = resultado1[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        if 'Arg' in r['variantes_dialectales']:\n",
    "            print(\"   ✅ DETECTÓ variante dialectal\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó variante dialectal\")\n",
    "    \n",
    "    # Caso 2: achiku - acepción + variante después de ||\n",
    "    print(\"\\n2. CASO achiku (acepción + variante después de ||):\")\n",
    "    entrada2 = \"achiku. adj. Estrafalario. || Gracioso. Arg: achika.\"\n",
    "    resultado2 = parsear_entrada_ultra_estricta(entrada2, abrev_test)\n",
    "    if resultado2:\n",
    "        r = resultado2[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        if \"Gracioso\" in r['definicion'] and 'Arg' in r['variantes_dialectales']:\n",
    "            print(\"   ✅ DETECTÓ acepción Y variante\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó correctamente\")\n",
    "    \n",
    "    # Caso 3: achikyay - variante dialectal compleja después de ||\n",
    "    print(\"\\n3. CASO achikyay (variante dialectal compleja después de ||):\")\n",
    "    entrada3 = \"achikyay. v. Rayar la aurora. Centellear, titilar las primeras luces del amanecer. || Pe.Apu: Aya: achij (luz, claridad, resplandor)\"\n",
    "    resultado3 = parsear_entrada_ultra_estricta(entrada3, abrev_test)\n",
    "    if resultado3:\n",
    "        r = resultado3[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        if 'Pe' in r['variantes_dialectales']:\n",
    "            print(\"   ✅ DETECTÓ variante dialectal compleja\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó variante dialectal compleja\")\n",
    "\n",
    "# Ejecutar prueba específica\n",
    "probar_casos_usuario()  # Test de casos específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44784e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando generación completa...\n",
      "\n",
      "=== TEST DE CASOS PROBLEMÁTICOS ===\n",
      "\n",
      "🧪 Caso de prueba 1:\n",
      "Entrada: achacha. s. Juguete. SINÓN: pukllana. Pe.Aya: pujllana. Arg: achala, achocha. Bo...\n",
      "   Acepción 1:\n",
      "     🏷️  Lema: 'achacha'\n",
      "     📝 Definición: 'Juguete. Vestido lujoso'\n",
      "     🔄 Sinónimos: ['pukllana']\n",
      "     🌍 Dialectales: {'Pe': {'Aya': ['pujllana']}, 'Arg': ['achala', 'achocha'], 'Bol': ['pukllana', 'phukllana']}\n",
      "\n",
      "🧪 Caso de prueba 2:\n",
      "Entrada: achachilla. s. Relig. Apacheta. || Ec: Veneración de los accidentes geográficos,...\n",
      "   Acepción 1:\n",
      "     🏷️  Lema: 'achachilla'\n",
      "     📝 Definición: 'Apacheta'\n",
      "     🔄 Sinónimos: ['apachita']\n",
      "     🌍 Dialectales: {'Ec': ['Veneración de los accidentes geográficos', 'considerados como lugares sagrados']}\n",
      "=== GENERACIÓN DE ARCHIVOS JSON ===\n",
      "Utilizando parsers de la Tarea 4\n",
      "\n",
      "✅ Abreviaturas cargadas:\n",
      "   - Categorías gramaticales: 46\n",
      "   - Campos semánticos: 93\n",
      "   - Dialectales: 18\n",
      "\n",
      "📖 Procesando sección Quechua-Español...\n",
      "Encontradas 16275 entradas potenciales\n",
      "Encontradas 16275 entradas potenciales\n",
      "   ✅ 10062 entradas procesadas\n",
      "\n",
      "📖 Procesando sección Español-Quechua...\n",
      "Encontradas 4577 entradas potenciales\n",
      "   ✅ 10062 entradas procesadas\n",
      "\n",
      "📖 Procesando sección Español-Quechua...\n",
      "Encontradas 4577 entradas potenciales\n",
      "   ✅ 4571 entradas procesadas\n",
      "   ✅ 4571 entradas procesadas\n",
      "\n",
      "💾 Guardado: quechua_espanol.json (10062 entradas)\n",
      "💾 Guardado: espanol_quechua.json (4571 entradas)\n",
      "\n",
      "⏱️  Tiempo total: 56.3 segundos\n",
      "\n",
      "=== RESUMEN FINAL ===\n",
      "\n",
      "📚 Quechua-Español: 10062 entradas\n",
      "   Ejemplo: 'A, a' → 'Primera letra o grafía y primera vocal del alfabet...'\n",
      "\n",
      "📚 Español-Quechua: 4571 entradas\n",
      "   Ejemplo: 'abajo' → 'Uray, ura, uran, lurin...'\n",
      "   Dialectales: {'Arg': ['ura']}\n",
      "\n",
      "🎯 TOTAL: 14633 entradas procesadas correctamente\n",
      "✅ Archivos JSON generados con campos separados según especificaciones\n",
      "\n",
      "🎉 PROCESO COMPLETADO EXITOSAMENTE\n",
      "\n",
      "💾 Guardado: quechua_espanol.json (10062 entradas)\n",
      "💾 Guardado: espanol_quechua.json (4571 entradas)\n",
      "\n",
      "⏱️  Tiempo total: 56.3 segundos\n",
      "\n",
      "=== RESUMEN FINAL ===\n",
      "\n",
      "📚 Quechua-Español: 10062 entradas\n",
      "   Ejemplo: 'A, a' → 'Primera letra o grafía y primera vocal del alfabet...'\n",
      "\n",
      "📚 Español-Quechua: 4571 entradas\n",
      "   Ejemplo: 'abajo' → 'Uray, ura, uran, lurin...'\n",
      "   Dialectales: {'Arg': ['ura']}\n",
      "\n",
      "🎯 TOTAL: 14633 entradas procesadas correctamente\n",
      "✅ Archivos JSON generados con campos separados según especificaciones\n",
      "\n",
      "🎉 PROCESO COMPLETADO EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# TAREA 5: Generación de archivos JSON estructurados\n",
    "\n",
    "import time\n",
    "\n",
    "def cargar_abreviaturas():\n",
    "    \"\"\"\n",
    "    Carga las abreviaturas desde el archivo JSON generado anteriormente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"abreviaturas.json\", 'r', encoding='utf-8') as f:\n",
    "            abreviaturas = json.load(f)\n",
    "        return abreviaturas\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: archivo abreviaturas.json no encontrado\")\n",
    "        return {\n",
    "            'categorias_gramaticales': ['s.', 'adj.', 'v.', 'adv.', 'interj.', 'prep.', 'conj.', 'pron.'],\n",
    "            'campos_semanticos': ['Bot.', 'Zool.', 'Med.', 'Hist.', 'Geog.'],\n",
    "            'dialectales': ['Arg.', 'Bol.', 'Pe.Anc.', 'Pe.Aya.', 'Pe.Qos.']\n",
    "        }\n",
    "\n",
    "def generar_archivos_json():\n",
    "    \"\"\"\n",
    "    Ejecuta los parsers y genera los archivos JSON según las especificaciones\n",
    "    \"\"\"\n",
    "    print(\"=== GENERACIÓN DE ARCHIVOS JSON ===\")\n",
    "    print(\"Utilizando parsers de la Tarea 4\\n\")\n",
    "    \n",
    "    # Cargar abreviaturas\n",
    "    abreviaturas = cargar_abreviaturas()\n",
    "    print(f\"✅ Abreviaturas cargadas:\")\n",
    "    print(f\"   - Categorías gramaticales: {len(abreviaturas['categorias_gramaticales'])}\")\n",
    "    print(f\"   - Campos semánticos: {len(abreviaturas['campos_semanticos'])}\")\n",
    "    print(f\"   - Dialectales: {len(abreviaturas['dialectales'])}\")\n",
    "    \n",
    "    inicio = time.time()\n",
    "    \n",
    "    # Crear parsers usando las clases de la Tarea 4\n",
    "    parser_qe = QuechuaEspanolParser(abreviaturas)\n",
    "    parser_eq = EspanolQuechuaParser(abreviaturas)\n",
    "    \n",
    "    # Procesar sección Quechua-Español\n",
    "    entradas_qe = []\n",
    "    if os.path.exists(\"seccion_quechua_espanol.txt\"):\n",
    "        print(\"\\n📖 Procesando sección Quechua-Español...\")\n",
    "        with open(\"seccion_quechua_espanol.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_qe = f.read()\n",
    "        \n",
    "        entradas_qe = parser_qe.parsear_seccion(texto_qe, max_entradas=15000)\n",
    "        print(f\"   ✅ {len(entradas_qe)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"   ❌ Archivo seccion_quechua_espanol.txt no encontrado\")\n",
    "    \n",
    "    # Procesar sección Español-Quechua\n",
    "    entradas_eq = []\n",
    "    if os.path.exists(\"seccion_espanol_quechua.txt\"):\n",
    "        print(\"\\n📖 Procesando sección Español-Quechua...\")\n",
    "        with open(\"seccion_espanol_quechua.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_eq = f.read()\n",
    "        \n",
    "        entradas_eq = parser_eq.parsear_seccion(texto_eq, max_entradas=15000)\n",
    "        print(f\"   ✅ {len(entradas_eq)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"   ❌ Archivo seccion_espanol_quechua.txt no encontrado\")\n",
    "    \n",
    "    # Guardar archivos JSON\n",
    "    if entradas_qe:\n",
    "        with open(\"quechua_espanol.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_qe, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n💾 Guardado: quechua_espanol.json ({len(entradas_qe)} entradas)\")\n",
    "    \n",
    "    if entradas_eq:\n",
    "        with open(\"espanol_quechua.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_eq, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"💾 Guardado: espanol_quechua.json ({len(entradas_eq)} entradas)\")\n",
    "    \n",
    "    tiempo_total = time.time() - inicio\n",
    "    print(f\"\\n⏱️  Tiempo total: {tiempo_total:.1f} segundos\")\n",
    "    \n",
    "    return entradas_qe, entradas_eq\n",
    "\n",
    "def test_casos_problematicos():\n",
    "    \"\"\"\n",
    "    Prueba los casos específicos mencionados por el usuario\n",
    "    \"\"\"\n",
    "    print(\"\\n=== TEST DE CASOS PROBLEMÁTICOS ===\")\n",
    "    \n",
    "    abreviaturas = cargar_abreviaturas()\n",
    "    \n",
    "    casos_test = [\n",
    "        \"achacha. s. Juguete. SINÓN: pukllana. Pe.Aya: pujllana. Arg: achala, achocha. Bol: pukllana, phukllana. || Vestido lujoso.\",\n",
    "        \"achachilla. s. Relig. Apacheta. || Ec: Veneración de los accidentes geográficos, considerados como lugares sagrados. SINÓN: apachita.\"\n",
    "    ]\n",
    "    \n",
    "    for i, caso in enumerate(casos_test, 1):\n",
    "        print(f\"\\n🧪 Caso de prueba {i}:\")\n",
    "        print(f\"Entrada: {caso[:80]}...\")\n",
    "        \n",
    "        resultados = parsear_entrada_ultra_estricta(caso, abreviaturas)\n",
    "        for j, resultado in enumerate(resultados, 1):\n",
    "            print(f\"   Acepción {j}:\")\n",
    "            print(f\"     🏷️  Lema: '{resultado['lema']}'\")\n",
    "            print(f\"     📝 Definición: '{resultado['definicion']}'\")\n",
    "            print(f\"     🔄 Sinónimos: {resultado['sinonimos']}\")\n",
    "            print(f\"     🌍 Dialectales: {resultado['variantes_dialectales']}\")\n",
    "\n",
    "def mostrar_resumen_final(entradas_qe, entradas_eq):\n",
    "    \"\"\"\n",
    "    Muestra un resumen final de los resultados\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RESUMEN FINAL ===\")\n",
    "    \n",
    "    if entradas_qe:\n",
    "        print(f\"\\n📚 Quechua-Español: {len(entradas_qe)} entradas\")\n",
    "        # Mostrar muestra de la primera entrada\n",
    "        if entradas_qe:\n",
    "            primera = entradas_qe[0]\n",
    "            print(f\"   Ejemplo: '{primera['lema']}' → '{primera['definicion'][:50]}...'\")\n",
    "            if primera['variantes_dialectales']:\n",
    "                print(f\"   Dialectales: {primera['variantes_dialectales']}\")\n",
    "    \n",
    "    if entradas_eq:\n",
    "        print(f\"\\n📚 Español-Quechua: {len(entradas_eq)} entradas\")\n",
    "        # Mostrar muestra de la primera entrada\n",
    "        if entradas_eq:\n",
    "            primera = entradas_eq[0]\n",
    "            print(f\"   Ejemplo: '{primera['lema']}' → '{primera['definicion'][:50]}...'\")\n",
    "            if primera['variantes_dialectales']:\n",
    "                print(f\"   Dialectales: {primera['variantes_dialectales']}\")\n",
    "    \n",
    "    total = len(entradas_qe) + len(entradas_eq)\n",
    "    print(f\"\\n🎯 TOTAL: {total} entradas procesadas correctamente\")\n",
    "    print(\"✅ Archivos JSON generados con campos separados según especificaciones\")\n",
    "\n",
    "# EJECUTAR TODO EL PROCESO\n",
    "print(\"Iniciando generación completa...\")\n",
    "\n",
    "# 1. Ejecutar test de casos problemáticos\n",
    "test_casos_problematicos()\n",
    "\n",
    "# 2. Generar archivos JSON\n",
    "entradas_qe, entradas_eq = generar_archivos_json()\n",
    "\n",
    "# 3. Mostrar resumen final\n",
    "if entradas_qe or entradas_eq:\n",
    "    mostrar_resumen_final(entradas_qe, entradas_eq)\n",
    "    print(\"\\n🎉 PROCESO COMPLETADO EXITOSAMENTE\")\n",
    "else:\n",
    "    print(\"\\n❌ Error en el proceso - verificar archivos de entrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d32af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRUEBAS DE CASOS ESPECÍFICOS ===\n",
      "\n",
      "1. CASO aa! (variante dialectal después de ||):\n",
      "   Definición: ¡Oh!, ¡ah! Arcaísmo de a\n",
      "   Variantes: {'Arg': ['Fuera', 'afuera']}\n",
      "   ✅ DETECTÓ variante dialectal\n",
      "\n",
      "2. CASO achiku (acepción + variante después de ||):\n",
      "   Definición: Estrafalario. Gracioso\n",
      "   Variantes: {'Arg': ['achika']}\n",
      "   ✅ DETECTÓ acepción Y variante\n",
      "\n",
      "3. CASO achikyay (variante dialectal compleja después de ||):\n",
      "   Definición: Rayar la aurora. Centellear, titilar las primeras luces del amanecer\n",
      "   Variantes: {'Pe': {'Apu': ['achij (luz', 'claridad', 'resplandor)'], 'Aya': ['achij (luz', 'claridad', 'resplandor)']}}\n",
      "   ✅ DETECTÓ variante dialectal compleja\n",
      "\n",
      "4. CASO achala (múltiples dialectos después de ||):\n",
      "   Definición: ¡Válgame Dios! ¡Jesús!, ¡Virgen Santa! ¡Cáspita! Exclamación de asombro por excelencia\n",
      "   Variantes: {'Pe': {'Caj': ['achal']}, 'Bol': ['achala']}\n",
      "   ✅ DETECTÓ múltiples variantes dialectales\n",
      "\n",
      "5. CASO alqo (múltiples categorías s. y adj.):\n",
      "   Categorías: ['s.', 'adj.']\n",
      "   Campo semántico: ['insult.']\n",
      "   Definición: Perro. Despectivo\n",
      "   ✅ DETECTÓ múltiples categorías\n"
     ]
    }
   ],
   "source": [
    "# Función de prueba para casos específicos del usuario\n",
    "def probar_casos_usuario():\n",
    "    \"\"\"Prueba casos específicos mencionados por el usuario\"\"\"\n",
    "    # Abreviaturas de prueba\n",
    "    abrev_test = {\n",
    "        'categorias_gramaticales': ['s.', 'adj.', 'v.', 'interj.'],\n",
    "        'campos_semanticos': ['Bot.', 'Med.Folk.', 'Ecol.Veg.', 'Zool.', 'Agri.', 'insult.'],\n",
    "        'dialectales': ['Pe.Aya.', 'Bol.', 'Ec.', 'Arg.', 'Pe.Apu.', 'Pe.Caj.', 'Pe.Jun.']\n",
    "    }\n",
    "    \n",
    "    print(\"=== PRUEBAS DE CASOS ESPECÍFICOS ===\")\n",
    "    \n",
    "    # Caso 1: aa! - variante dialectal después de ||\n",
    "    print(\"\\n1. CASO aa! (variante dialectal después de ||):\")\n",
    "    entrada1 = \"aa! interj. ¡Oh!, ¡ah! Arcaísmo de a! || Arg: Fuera, afuera.\"\n",
    "    resultado1 = parsear_entrada_ultra_estricta(entrada1, abrev_test)\n",
    "    if resultado1:\n",
    "        r = resultado1[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        if 'Arg' in r['variantes_dialectales']:\n",
    "            print(\"   ✅ DETECTÓ variante dialectal\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó variante dialectal\")\n",
    "    \n",
    "    # Caso 2: achiku - acepción + variante después de ||\n",
    "    print(\"\\n2. CASO achiku (acepción + variante después de ||):\")\n",
    "    entrada2 = \"achiku. adj. Estrafalario. || Gracioso. Arg: achika.\"\n",
    "    resultado2 = parsear_entrada_ultra_estricta(entrada2, abrev_test)\n",
    "    if resultado2:\n",
    "        r = resultado2[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        if \"Gracioso\" in r['definicion'] and 'Arg' in r['variantes_dialectales']:\n",
    "            print(\"   ✅ DETECTÓ acepción Y variante\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó correctamente\")\n",
    "    \n",
    "    # Caso 3: achikyay - variante dialectal compleja después de ||\n",
    "    print(\"\\n3. CASO achikyay (variante dialectal compleja después de ||):\")\n",
    "    entrada3 = \"achikyay. v. Rayar la aurora. Centellear, titilar las primeras luces del amanecer. || Pe.Apu: Aya: achij (luz, claridad, resplandor)\"\n",
    "    resultado3 = parsear_entrada_ultra_estricta(entrada3, abrev_test)\n",
    "    if resultado3:\n",
    "        r = resultado3[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        if 'Pe' in r['variantes_dialectales']:\n",
    "            print(\"   ✅ DETECTÓ variante dialectal compleja\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó variante dialectal compleja\")\n",
    "\n",
    "    # Casos adicionales mencionados por el usuario\n",
    "    print(\"\\n4. CASO achala (múltiples dialectos después de ||):\")\n",
    "    entrada4 = \"achala. interj. ¡Válgame Dios! ¡Jesús!, ¡Virgen Santa! ¡Cáspita! Exclamación de asombro por excelencia. || Pe.Caj: achal. Bol: achala\"\n",
    "    resultado4 = parsear_entrada_ultra_estricta(entrada4, abrev_test)\n",
    "    if resultado4:\n",
    "        r = resultado4[0]\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        print(f\"   Variantes: {r['variantes_dialectales']}\")\n",
    "        esperado = len(['Pe', 'Bol']) \n",
    "        encontrado = len(r['variantes_dialectales'])\n",
    "        if encontrado >= 2:\n",
    "            print(\"   ✅ DETECTÓ múltiples variantes dialectales\")\n",
    "        else:\n",
    "            print(f\"   ❌ Solo detectó {encontrado} variantes, esperaba al menos 2\")\n",
    "\n",
    "    # Casos con múltiples categorías\n",
    "    print(\"\\n5. CASO alqo (múltiples categorías s. y adj.):\")\n",
    "    entrada5 = \"alqo. s. Perro. || adj. insult. Despectivo\"\n",
    "    resultado5 = parsear_entrada_ultra_estricta(entrada5, abrev_test)\n",
    "    if resultado5:\n",
    "        r = resultado5[0]\n",
    "        print(f\"   Categorías: {r['categoria_gramatical']}\")\n",
    "        print(f\"   Campo semántico: {r['campo_semantico']}\")\n",
    "        print(f\"   Definición: {r['definicion']}\")\n",
    "        if isinstance(r['categoria_gramatical'], list) and len(r['categoria_gramatical']) >= 2:\n",
    "            print(\"   ✅ DETECTÓ múltiples categorías\")\n",
    "        else:\n",
    "            print(\"   ❌ NO detectó múltiples categorías\")\n",
    "\n",
    "# Ejecutar prueba específica\n",
    "probar_casos_usuario()  # Test de casos específicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3544273",
   "metadata": {},
   "source": [
    "## RESUMEN Y USO DE LA LIBRERÍA\n",
    "\n",
    "### Archivos Generados\n",
    "1. **diccionario_raw.txt** - Texto crudo extraído del PDF\n",
    "2. **abreviaturas.json** - Abreviaturas categorizadas\n",
    "3. **quechua_espanol.json** - Entradas Q→E estructuradas\n",
    "4. **espanol_quechua.json** - Entradas E→Q estructuradas\n",
    "5. **diccionario_utils.py** - Librería Python de consultas\n",
    "\n",
    "### Uso de la Librería\n",
    "\n",
    "```python\n",
    "from diccionario_utils import cargar_diccionario\n",
    "\n",
    "# Cargar diccionario\n",
    "diccionario = cargar_diccionario()\n",
    "\n",
    "# Búsquedas básicas\n",
    "resultados = diccionario.buscar_por_quechua(\"achupalla\")\n",
    "variantes = diccionario.obtener_variantes_dialectales(\"puya\")\n",
    "\n",
    "# Filtros especializados\n",
    "botanicos = diccionario.buscar_por_campo_semantico(\"Bot.\")\n",
    "sustantivos = diccionario.buscar_por_categoria_gramatical(\"s.\")\n",
    "\n",
    "# Estadísticas\n",
    "stats = diccionario.estadisticas()\n",
    "```\n",
    "\n",
    "### Características del Sistema\n",
    "- ✅ **Extracción automática** desde PDF\n",
    "- ✅ **Parseo estructurado** con regex optimizadas\n",
    "- ✅ **API de consulta** completa y eficiente\n",
    "- ✅ **Validación integrada** con tests automatizados\n",
    "- ✅ **Documentación completa** y ejemplos de uso\n",
    "\n",
    "### Impacto\n",
    "Este proyecto sienta las bases para el desarrollo de herramientas de PLN para Quechua, una lengua indígena hablada por millones de personas en los Andes centrales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
