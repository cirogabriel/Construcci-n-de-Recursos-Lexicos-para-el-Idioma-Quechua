{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f7bcf4",
   "metadata": {},
   "source": [
    "# CONSTRUCCIÓN DE RECURSOS LÉXICOS PARA QUECHUA\n",
    "\n",
    "## Objetivo\n",
    "Extraer, estructurar y enriquecer el diccionario bilingüe Quechua-Español para crear un corpus léxico machine-readable.\n",
    "\n",
    "## Estructura del Proyecto\n",
    "- **Entrada**: Diccionario PDF bilingüe (Quechua-Español / Español-Quechua)\n",
    "- **Salida**: Archivos JSON estructurados + librería Python para consultas\n",
    "\n",
    "## Tareas\n",
    "1. Extracción de texto crudo del PDF\n",
    "2. Identificación y separación de secciones\n",
    "3. Diseño e implementación de parsers\n",
    "4. Generación de archivos JSON estructurados\n",
    "5. Desarrollo de librería de utilidades\n",
    "6. Validación y testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dfd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías instaladas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías necesarias\n",
    "# !pip install PyMuPDF pdfplumber regex\n",
    "\n",
    "# Importaciones\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Librerías instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940bb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo texto del PDF...\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extraído: 1930001 caracteres\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extraído: 1930001 caracteres\n"
     ]
    }
   ],
   "source": [
    "# TAREA 1 y 2: Extracción de texto del PDF y separación de secciones\n",
    "\n",
    "def extraer_texto_pdf(ruta_pdf: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae texto crudo del PDF preservando estructura básica sin alteraciones\n",
    "    \"\"\"\n",
    "    texto_completo = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Usar PyMuPDF para extracción\n",
    "        doc = fitz.open(ruta_pdf)\n",
    "        \n",
    "        for pagina_num in range(doc.page_count):\n",
    "            pagina = doc[pagina_num]\n",
    "            texto = pagina.get_text()\n",
    "            texto_completo += texto\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error con PyMuPDF: {e}\")\n",
    "        \n",
    "        # Alternativa con pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(ruta_pdf) as pdf:\n",
    "                for pagina in pdf.pages:\n",
    "                    texto = pagina.extract_text()\n",
    "                    if texto:\n",
    "                        texto_completo += texto\n",
    "        except Exception as e2:\n",
    "            print(f\"Error con pdfplumber: {e2}\")\n",
    "            return \"\"\n",
    "    \n",
    "    return texto_completo\n",
    "\n",
    "def guardar_texto_crudo(texto: str, ruta_salida: str = \"diccionario_raw.txt\"):\n",
    "    \"\"\"\n",
    "    Guarda el texto crudo extraído del PDF\n",
    "    \"\"\"\n",
    "    with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "        f.write(texto)\n",
    "    print(f\"Texto crudo guardado en: {ruta_salida}\")\n",
    "\n",
    "# Ejecutar extracción\n",
    "ruta_pdf = \"diccionario-qeswa-academia-mayor.pdf\"\n",
    "if os.path.exists(ruta_pdf):\n",
    "    print(\"Extrayendo texto del PDF...\")\n",
    "    texto_crudo = extraer_texto_pdf(ruta_pdf)\n",
    "    guardar_texto_crudo(texto_crudo)\n",
    "    print(f\"Texto extraído: {len(texto_crudo)} caracteres\")\n",
    "else:\n",
    "    print(f\"Archivo PDF no encontrado: {ruta_pdf}\")\n",
    "    print(\"Por favor, asegúrese de que el archivo esté en el directorio correcto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d73b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando secciones del diccionario...\n",
      "Sección Quechua-Español: 48883 líneas\n",
      "Sección Español-Quechua: 9264 líneas\n",
      "Archivos guardados correctamente\n",
      "Sección Quechua-Español: 48883 líneas\n",
      "Sección Español-Quechua: 9264 líneas\n",
      "Archivos guardados correctamente\n"
     ]
    }
   ],
   "source": [
    "# TAREA 3: Extracción y limpieza de secciones del diccionario\n",
    "\n",
    "def limpiar_encabezados_y_patrones(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando encabezados de página, títulos, letras de sección y otros patrones no deseados.\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    lineas_limpias = []\n",
    "    \n",
    "    for linea in lineas:\n",
    "        linea_original = linea\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Ignorar líneas vacías\n",
    "        if not linea:\n",
    "            continue\n",
    "        \n",
    "        # PATRONES A ELIMINAR:\n",
    "        \n",
    "        # 1. Números de página junto con DICCIONARIO\n",
    "        if re.match(r'^\\d+$', linea) or 'DICCIONARIO' in linea.upper():\n",
    "            continue\n",
    "        \n",
    "        # 2. Símbolos decorativos como ◄●►\n",
    "        if re.match(r'^[◄●►\\-=\\*\\+\\s]+$', linea):\n",
    "            continue\n",
    "        \n",
    "        # 3. Título SIMI TAQE\n",
    "        if 'SIMI TAQE' in linea.upper():\n",
    "            continue\n",
    "        \n",
    "        # 4. Letras sueltas de sección (una sola letra mayúscula en su propia línea)\n",
    "        if re.match(r'^[A-Z]$', linea):\n",
    "            continue\n",
    "        \n",
    "        # 5. Encabezados típicos de PDF\n",
    "        if any(patron in linea.upper() for patron in [\n",
    "            'PÁGINA', 'CAPÍTULO', 'SECCIÓN', 'ÍNDICE', 'CONTENIDO',\n",
    "            'DICCIONARIO QUECHUA', 'ESPAÑOL - QUECHUA', 'QUECHUA - ESPAÑOL'\n",
    "        ]):\n",
    "            continue\n",
    "        \n",
    "        # 6. Líneas que son solo números o símbolos\n",
    "        if re.match(r'^[\\d\\s\\-\\.\\*\\+◄●►]+$', linea):\n",
    "            continue\n",
    "        \n",
    "        # 7. Líneas muy cortas que no parecen ser contenido útil (menos de 3 caracteres)\n",
    "        if len(linea) < 3:\n",
    "            continue\n",
    "        \n",
    "        # 8. Patrones específicos de encabezados de página\n",
    "        if re.match(r'^\\d+\\s*◄●►\\s*$', linea) or re.match(r'^◄●►\\s*\\d+\\s*$', linea):\n",
    "            continue\n",
    "        \n",
    "        # Si la línea pasó todos los filtros, mantenerla\n",
    "        lineas_limpias.append(linea_original)\n",
    "    \n",
    "    return '\\n'.join(lineas_limpias)\n",
    "\n",
    "def separar_secciones_por_lineas(texto: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Separa las secciones usando los números de línea conocidos y aplica limpieza de encabezados\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    \n",
    "    # Sección Quechua-Español: línea 1325 a 50998\n",
    "    seccion_qe_lineas = lineas[1325:50998]\n",
    "    \n",
    "    # Limpiar texto extra al final de la sección Quechua-Español\n",
    "    seccion_qe_contenido = []\n",
    "    for linea in seccion_qe_lineas:\n",
    "        if \"ESPAÑOL - QUECHUA\" in linea:\n",
    "            break\n",
    "        seccion_qe_contenido.append(linea)\n",
    "    \n",
    "    # Sección Español-Quechua: línea 50998 hasta el final\n",
    "    seccion_eq_contenido = []\n",
    "    for i in range(50998, len(lineas)):\n",
    "        linea = lineas[i].strip()\n",
    "        if (\"DICCIONARIO QUECHUA – ESPAÑOL – QUECHUA\" in linea or \n",
    "            \"se terminó de imprimir\" in linea or \n",
    "            \"talleres gráficos\" in linea):\n",
    "            break\n",
    "        seccion_eq_contenido.append(lineas[i])\n",
    "    \n",
    "    # Aplicar limpieza de encabezados y patrones no deseados\n",
    "    seccion_qe_texto = '\\n'.join(seccion_qe_contenido)\n",
    "    seccion_eq_texto = '\\n'.join(seccion_eq_contenido)\n",
    "    \n",
    "    seccion_qe_limpia = limpiar_encabezados_y_patrones(seccion_qe_texto)\n",
    "    seccion_eq_limpia = limpiar_encabezados_y_patrones(seccion_eq_texto)\n",
    "    \n",
    "    return seccion_qe_limpia, seccion_eq_limpia\n",
    "\n",
    "# Ejecutar extracción y limpieza de secciones\n",
    "if os.path.exists(\"diccionario_raw.txt\"):\n",
    "    with open(\"diccionario_raw.txt\", 'r', encoding='utf-8') as f:\n",
    "        texto_completo = f.read()\n",
    "    \n",
    "    print(\"Procesando secciones del diccionario...\")\n",
    "    \n",
    "    seccion_qe, seccion_eq = separar_secciones_por_lineas(texto_completo)\n",
    "    \n",
    "    # Contar líneas aproximadas para verificar el contenido\n",
    "    lineas_qe = [l for l in seccion_qe.split('\\n') if l.strip()]\n",
    "    lineas_eq = [l for l in seccion_eq.split('\\n') if l.strip()]\n",
    "    \n",
    "    print(f\"Sección Quechua-Español: {len(lineas_qe)} líneas\")\n",
    "    print(f\"Sección Español-Quechua: {len(lineas_eq)} líneas\")\n",
    "    \n",
    "    # Guardar archivos limpios\n",
    "    with open(\"seccion_quechua_espanol.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_qe)\n",
    "    \n",
    "    with open(\"seccion_espanol_quechua.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_eq)\n",
    "    \n",
    "    print(\"Archivos guardados correctamente\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: archivo diccionario_raw.txt no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcdafa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAREA 4: Diseño e implementación de parsers\n",
    "\n",
    "def extraer_entradas_completas(texto: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrae entradas completas del texto, incluyendo las que abarcan múltiples líneas.\n",
    "    Mejorado para manejar lemas compuestos (ej: \"Amaru Inka Yupanki\") y apostrofes (ej: \"map'a\")\n",
    "    CORRIGE: Problema de absorción de lemas siguientes con remisiones V.\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    entradas = []\n",
    "    entrada_actual = \"\"\n",
    "    \n",
    "    # Patrón mejorado que incluye apostrofes y detecta inicios de entrada\n",
    "    patron_inicio = r'^[a-zA-ZÀ-ÿñÑ!¡¿?,\\s\\']+[\\.!]\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.)'\n",
    "    \n",
    "    for linea in lineas:\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Verificar si es el inicio de una nueva entrada\n",
    "        if re.match(patron_inicio, linea):\n",
    "            # Guardar entrada anterior si existe\n",
    "            if entrada_actual.strip():\n",
    "                entradas.append(entrada_actual.strip())\n",
    "            # Iniciar nueva entrada\n",
    "            entrada_actual = linea\n",
    "        else:\n",
    "            # Continuar entrada actual\n",
    "            if entrada_actual:\n",
    "                entrada_actual += \" \" + linea\n",
    "    \n",
    "    # Agregar la última entrada\n",
    "    if entrada_actual.strip():\n",
    "        entradas.append(entrada_actual.strip())\n",
    "    \n",
    "    # POST-PROCESAMIENTO: Separar entradas que fueron incorrectamente unidas\n",
    "    entradas_corregidas = []\n",
    "    \n",
    "    for entrada in entradas:\n",
    "        # Buscar patrones de lemas que fueron absorbidos incorrectamente\n",
    "        # Patrón: texto completo + lema. categoria V. REFERENCIA\n",
    "        patron_absorcion = r'^(.+?)\\s+([a-zA-ZÀ-ÿñÑ!¡¿?,\\s\\']+[\\.!])\\s+(V\\.\\s+[A-ZÁÉÍÓÚÑ\\s]+\\.?)\\s*$'\n",
    "        match = re.match(patron_absorcion, entrada)\n",
    "        \n",
    "        if match:\n",
    "            # Separar las entradas\n",
    "            entrada_principal = match.group(1).strip()\n",
    "            lema_absorbido = match.group(2).strip()\n",
    "            remision = match.group(3).strip()\n",
    "            \n",
    "            # Verificar que el lema absorbido es realmente un lema válido\n",
    "            if re.match(r'^[a-zA-ZÀ-ÿñÑ!¡¿?,\\s\\']+[\\.!]$', lema_absorbido):\n",
    "                entradas_corregidas.append(entrada_principal)\n",
    "                entradas_corregidas.append(f\"{lema_absorbido} {remision}\")\n",
    "            else:\n",
    "                entradas_corregidas.append(entrada)\n",
    "        else:\n",
    "            entradas_corregidas.append(entrada)\n",
    "    \n",
    "    return entradas_corregidas\n",
    "\n",
    "def parsear_entrada_ultra_estricta(entrada_texto: str, abreviaturas: Dict, es_espanol: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parser ULTRA-ESTRICTO mejorado según todas las especificaciones del usuario.\n",
    "    Maneja: lemas compuestos, remisiones (V.), definiciones múltiples, || como separador de acepciones/dialectales.\n",
    "    \"\"\"\n",
    "    if len(entrada_texto) < 10:\n",
    "        return []\n",
    "    \n",
    "    # 1. EXTRAER LEMA (puede ser compuesto: \"Amaru Inka Yupanki\")\n",
    "    # Patrón mejorado para lemas compuestos\n",
    "    match_lema = re.match(r'^([a-zA-ZÀ-ÿñÑ!¡¿?,\\s]+?)[\\.!]\\s', entrada_texto)\n",
    "    if not match_lema:\n",
    "        return []\n",
    "    \n",
    "    lema = match_lema.group(1).strip()\n",
    "    # Mantener mayúsculas para lemas compuestos (ej: \"Amaru Inka Yupanki\")\n",
    "    # Solo convertir a minúscula si es una sola palabra que no empieza con mayúscula\n",
    "    if ' ' not in lema and not lema[0].isupper():\n",
    "        lema = lema.lower()\n",
    "    \n",
    "    # Extraer resto del texto después del lema\n",
    "    resto = entrada_texto[len(match_lema.group(0)):].strip()\n",
    "    \n",
    "    # Filtro para español: evitar palabras quechuas\n",
    "    if es_espanol:\n",
    "        if any(c in lema.lower() for c in ['k', 'w', 'q']) and 'qu' not in lema.lower():\n",
    "            return []\n",
    "    \n",
    "    # Obtener listas de abreviaturas\n",
    "    categorias_gramaticales = abreviaturas.get('categorias_gramaticales', [])\n",
    "    campos_semanticos = abreviaturas.get('campos_semanticos', [])\n",
    "    dialectales = abreviaturas.get('dialectales', [])\n",
    "    \n",
    "    # 2. EXTRAER CATEGORÍA GRAMATICAL (inmediatamente después del lema)\n",
    "    categoria = ''\n",
    "    for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "        if resto.startswith(cat):\n",
    "            categoria = cat\n",
    "            resto = resto[len(cat):].strip()\n",
    "            break\n",
    "    \n",
    "    if not categoria:\n",
    "        return []\n",
    "    \n",
    "    # 3. DETECTAR TODOS LOS CAMPOS SEMÁNTICOS EN TODA LA DEFINICIÓN (REGLA 1)\n",
    "    campos_encontrados = []\n",
    "    texto_busqueda = resto\n",
    "    \n",
    "    # Buscar TODOS los campos semánticos en todo el texto, no solo al inicio\n",
    "    for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "        # Buscar en diferentes posiciones: inicio, después de punto, en medio\n",
    "        patrones_busqueda = [\n",
    "            f'^{re.escape(campo)}\\\\s',          # Al inicio\n",
    "            f'\\\\s{re.escape(campo)}\\\\s',        # En medio del texto\n",
    "            f'\\\\.\\\\s*{re.escape(campo)}\\\\s',    # Después de punto\n",
    "            f'^{re.escape(campo)}\\\\.',          # Al inicio con punto\n",
    "            f'\\\\s{re.escape(campo)}\\\\.'         # En medio con punto\n",
    "        ]\n",
    "        \n",
    "        for patron in patrones_busqueda:\n",
    "            if re.search(patron, texto_busqueda):\n",
    "                if campo not in campos_encontrados:\n",
    "                    campos_encontrados.append(campo)\n",
    "                break\n",
    "    \n",
    "    # 4. PROCESAMIENTO MEJORADO DE CONTENIDO DESPUÉS DE ||\n",
    "    if '||' in resto:\n",
    "        partes_separadas = [a.strip() for a in resto.split('||') if a.strip()]\n",
    "    else:\n",
    "        partes_separadas = [resto]\n",
    "    \n",
    "    # Estructura única de entrada que iremos completando\n",
    "    entrada = {\n",
    "        'lema': lema,\n",
    "        'categoria_gramatical': [categoria],  # Lista para múltiples categorías\n",
    "        'campo_semantico': campos_encontrados.copy(),\n",
    "        'definicion': '',\n",
    "        'variantes_dialectales': {},\n",
    "        'sinonimos': [],\n",
    "        'ejemplos': []\n",
    "    }\n",
    "    \n",
    "    # Lista para acumular todas las definiciones\n",
    "    todas_definiciones = []\n",
    "    \n",
    "    for i, parte in enumerate(partes_separadas):\n",
    "        texto = parte.strip()\n",
    "        \n",
    "        # 5. DETECTAR REMISIONES \"V. LEMA\" (véase)\n",
    "        match_remision = re.search(r'\\bV\\.\\s+([A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*)', texto)\n",
    "        if match_remision:\n",
    "            remite_a = match_remision.group(1).strip().lower()\n",
    "            entrada['remite_a'] = remite_a\n",
    "            texto = re.sub(r'\\bV\\.\\s+[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*', '', texto).strip()\n",
    "        \n",
    "        # 6. DETECTAR CATEGORÍA GRAMATICAL ADICIONAL (para casos como \"|| adj. insult.\")\n",
    "        categoria_adicional = ''\n",
    "        for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "            if texto.startswith(cat):\n",
    "                categoria_adicional = cat\n",
    "                if cat not in entrada['categoria_gramatical']:\n",
    "                    entrada['categoria_gramatical'].append(cat)\n",
    "                texto = texto[len(cat):].strip()\n",
    "                break\n",
    "        \n",
    "        # 7. DETECTAR CAMPOS SEMÁNTICOS ADICIONALES (para casos como \"|| Agri.\")\n",
    "        campos_adicionales = []\n",
    "        for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "            if texto.startswith(campo):\n",
    "                campos_adicionales.append(campo)\n",
    "                if campo not in entrada['campo_semantico']:\n",
    "                    entrada['campo_semantico'].append(campo)\n",
    "                texto = texto[len(campo):].strip()\n",
    "                break\n",
    "        \n",
    "        # 8. DETERMINAR TIPO DE CONTENIDO DESPUÉS DE ||\n",
    "        es_dialectal = False\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            if (f'{dialecto_sin_punto}:' in texto or \n",
    "                texto.startswith(dialecto_sin_punto + ':') or\n",
    "                f' {dialecto_sin_punto}:' in texto):\n",
    "                es_dialectal = True\n",
    "                break\n",
    "        \n",
    "        # 9. EXTRAER EJEMPLOS (EJEM:)\n",
    "        if 'EJEM:' in texto:\n",
    "            pos_ejem = texto.find('EJEM:')\n",
    "            texto_antes = texto[:pos_ejem].strip()\n",
    "            texto_ejem = texto[pos_ejem + 5:].strip()\n",
    "            \n",
    "            # Encontrar final del ejemplo (hasta dialecto o final)\n",
    "            fin_ejemplo = len(texto_ejem)\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                pos_dialecto = texto_ejem.find(f'{dialecto_sin_punto}:')\n",
    "                if pos_dialecto != -1 and pos_dialecto < fin_ejemplo:\n",
    "                    fin_ejemplo = pos_dialecto\n",
    "            \n",
    "            ejemplo_texto = texto_ejem[:fin_ejemplo].strip(' .,!')\n",
    "            if ejemplo_texto and ejemplo_texto not in entrada['ejemplos']:\n",
    "                entrada['ejemplos'].append(ejemplo_texto)\n",
    "            \n",
    "            # Continuar con el resto después del ejemplo\n",
    "            resto_ejemplo = texto_ejem[fin_ejemplo:].strip()\n",
    "            texto = texto_antes + ' ' + resto_ejemplo\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 8. EXTRAER SINÓNIMOS (SINÓN:) \n",
    "        if 'SINÓN:' in texto:\n",
    "            pos_sinon = texto.find('SINÓN:')\n",
    "            texto_antes = texto[:pos_sinon].strip()\n",
    "            texto_sinon = texto[pos_sinon + 6:].strip()\n",
    "            \n",
    "            # Los sinónimos terminan en el PRIMER punto seguido de mayúscula O dialecto\n",
    "            fin_sinon = len(texto_sinon)\n",
    "            \n",
    "            # Buscar primer punto seguido de mayúscula\n",
    "            for j, char in enumerate(texto_sinon):\n",
    "                if char == '.' and j + 1 < len(texto_sinon):\n",
    "                    siguiente = texto_sinon[j + 1:j + 10].strip()\n",
    "                    if siguiente and siguiente[0].isupper():\n",
    "                        fin_sinon = j + 1\n",
    "                        break\n",
    "            \n",
    "            # Buscar abreviaturas dialectales que interrumpen sinónimos\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                for patron in [f' {dialecto_sin_punto}:', f' {dialecto_sin_punto} ']:\n",
    "                    pos = texto_sinon.find(patron)\n",
    "                    if pos != -1 and pos < fin_sinon:\n",
    "                        fin_sinon = pos\n",
    "            \n",
    "            sinon_texto = texto_sinon[:fin_sinon].strip(' .,!')\n",
    "            if sinon_texto:\n",
    "                sinonimos = [s.strip() for s in sinon_texto.split(',') if s.strip()]\n",
    "                entrada['sinonimos'] = [re.sub(r'[^\\w\\sñÑáéíóúÁÉÍÓÚ]', '', s).strip() \n",
    "                                      for s in sinonimos if len(s.strip()) > 1]\n",
    "            \n",
    "            # Continuar con el resto después de los sinónimos\n",
    "            resto_despues_sinon = texto_sinon[fin_sinon:].strip()\n",
    "            texto = texto_antes + ' ' + resto_despues_sinon\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 11. EXTRAER VARIANTES DIALECTALES (MEJORADO PARA ||)\n",
    "        texto_limpio = texto\n",
    "        dialectales_ordenados = sorted(dialectales, key=len, reverse=True)\n",
    "        \n",
    "        for dialecto in dialectales_ordenados:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            \n",
    "            # Buscar patrón más flexible: \"Arg:\", \" Arg:\", al inicio o después de espacios\n",
    "            patrones_dialectal = [\n",
    "                f'{dialecto_sin_punto}:',      # Al inicio: \"Arg:\"\n",
    "                f' {dialecto_sin_punto}:',     # En medio: \" Arg:\"\n",
    "                f'^{dialecto_sin_punto}:'      # Exactamente al inicio\n",
    "            ]\n",
    "            \n",
    "            pos_encontrada = -1\n",
    "            patron_usado = ''\n",
    "            \n",
    "            for patron in patrones_dialectal:\n",
    "                if patron.startswith('^'):\n",
    "                    # Patrón de inicio exacto\n",
    "                    if texto_limpio.startswith(patron[1:]):\n",
    "                        pos_encontrada = 0\n",
    "                        patron_usado = patron[1:]\n",
    "                        break\n",
    "                else:\n",
    "                    pos = texto_limpio.find(patron)\n",
    "                    if pos != -1:\n",
    "                        pos_encontrada = pos\n",
    "                        patron_usado = patron\n",
    "                        break\n",
    "            \n",
    "            if pos_encontrada != -1:\n",
    "                texto_antes = texto_limpio[:pos_encontrada].strip()\n",
    "                texto_despues = texto_limpio[pos_encontrada + len(patron_usado):].strip()\n",
    "                \n",
    "                # Encontrar final de variantes dialectales\n",
    "                fin_dialectal = len(texto_despues)\n",
    "                \n",
    "                # Buscar próximo dialecto\n",
    "                for otro_dialecto in dialectales_ordenados:\n",
    "                    if otro_dialecto != dialecto:\n",
    "                        otro_sin_punto = otro_dialecto.rstrip('.')\n",
    "                        for patron_otro in [f'{otro_sin_punto}:', f' {otro_sin_punto}:']:\n",
    "                            pos_otro = texto_despues.find(patron_otro)\n",
    "                            if pos_otro != -1 and pos_otro < fin_dialectal:\n",
    "                                fin_dialectal = pos_otro\n",
    "                \n",
    "                # Buscar marcadores que interrumpen\n",
    "                for marcador in [' SINÓN:', ' EJEM:', '. SINÓN:', '. EJEM:']:\n",
    "                    pos_marcador = texto_despues.find(marcador)\n",
    "                    if pos_marcador != -1 and pos_marcador < fin_dialectal:\n",
    "                        fin_dialectal = pos_marcador\n",
    "                \n",
    "                contenido_dialectal = texto_despues[:fin_dialectal].strip(' .,!')\n",
    "                \n",
    "                if contenido_dialectal:\n",
    "                    # ANALIZAR CONTENIDO: formato \"Anc: Caj: pujllana\" o \"achala, achocha\"\n",
    "                    regiones_adicionales = []\n",
    "                    variantes_finales = []\n",
    "                    \n",
    "                    # Separar por dos puntos para detectar regiones comprimidas\n",
    "                    partes_dos_puntos = contenido_dialectal.split(':')\n",
    "                    \n",
    "                    if len(partes_dos_puntos) > 1:\n",
    "                        # Hay formato comprimido: \"Anc: Caj: pujllana\"\n",
    "                        for parte in partes_dos_puntos[:-1]:  # Todas excepto la última\n",
    "                            parte = parte.strip()\n",
    "                            # Verificar si es una región válida (3-4 letras, empieza con mayúscula)\n",
    "                            if re.match(r'^[A-Z][a-z]{2,3}$', parte):\n",
    "                                regiones_adicionales.append(parte)\n",
    "                        \n",
    "                        # La última parte son las variantes\n",
    "                        variantes_texto = partes_dos_puntos[-1].strip()\n",
    "                        variantes_finales = [v.strip() for v in variantes_texto.split(',') if v.strip()]\n",
    "                    else:\n",
    "                        # Formato simple: \"achala, achocha\"\n",
    "                        variantes_finales = [v.strip() for v in contenido_dialectal.split(',') if v.strip()]\n",
    "                    \n",
    "                    # CREAR ESTRUCTURA DE VARIANTES DIALECTALES (SIN \"Gen\")\n",
    "                    if '.' in dialecto and len(dialecto.split('.', 1)[1].rstrip('.')) > 0:\n",
    "                        # Caso: Pe.Aya. → pais='Pe', region='Aya'\n",
    "                        pais, region_principal = dialecto.split('.', 1)\n",
    "                        region_principal = region_principal.rstrip('.')\n",
    "                        \n",
    "                        if pais not in entrada['variantes_dialectales']:\n",
    "                            entrada['variantes_dialectales'][pais] = {}\n",
    "                        \n",
    "                        # Agregar región principal\n",
    "                        entrada['variantes_dialectales'][pais][region_principal] = variantes_finales\n",
    "                        \n",
    "                        # Agregar regiones adicionales con las mismas variantes\n",
    "                        for region_adicional in regiones_adicionales:\n",
    "                            entrada['variantes_dialectales'][pais][region_adicional] = variantes_finales\n",
    "                            \n",
    "                    else:\n",
    "                        # Caso: Arg. → pais='Arg', sin estructura anidada\n",
    "                        pais = dialecto.rstrip('.')\n",
    "                        entrada['variantes_dialectales'][pais] = variantes_finales\n",
    "                \n",
    "                # Limpiar del texto principal\n",
    "                texto_limpio = texto_antes + ' ' + texto_despues[fin_dialectal:]\n",
    "                texto_limpio = texto_limpio.strip()\n",
    "        \n",
    "        # 10. EXTRAER DEFINICIÓN (lo que queda después de limpiar todo)\n",
    "        definicion = texto_limpio\n",
    "        \n",
    "        # REGLA 1: Limpiar TODOS los campos semánticos detectados de la definición\n",
    "        for campo in campos_encontrados:\n",
    "            # Limpiar campo al inicio de la definición\n",
    "            definicion = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion)\n",
    "            # Limpiar campo en medio de la definición\n",
    "            definicion = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion)\n",
    "            # Limpiar campo después de punto\n",
    "            definicion = re.sub(f'\\\\.\\\\s*{re.escape(campo)}\\\\s*', '. ', definicion)\n",
    "        \n",
    "        # Limpiar campos adicionales encontrados después de ||\n",
    "        for campo in campos_adicionales:\n",
    "            definicion = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion)\n",
    "            definicion = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion)\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion = re.sub(r'SINÓN:.*$', '', definicion)\n",
    "        definicion = re.sub(r'EJEM:.*$', '', definicion)\n",
    "        definicion = re.sub(r'VARIEDADES:.*$', '', definicion, flags=re.IGNORECASE)\n",
    "        definicion = re.sub(r'\\bV\\.\\s+[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\\s]*', '', definicion)\n",
    "        \n",
    "        # Limpiar dialectales residuales\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            definicion = re.sub(f'{re.escape(dialecto_sin_punto)}:.*$', '', definicion)\n",
    "        \n",
    "        # 12. EXTRAER DEFINICIÓN (lo que queda después de limpiar todo)\n",
    "        definicion_parte = texto_limpio\n",
    "        \n",
    "        # Limpiar campos semánticos del texto\n",
    "        for campo in entrada['campo_semantico']:\n",
    "            definicion_parte = re.sub(f'^{re.escape(campo)}\\\\s*', '', definicion_parte)\n",
    "            definicion_parte = re.sub(f'\\\\s*{re.escape(campo)}\\\\s*', ' ', definicion_parte)\n",
    "            definicion_parte = re.sub(f'\\\\.\\\\s*{re.escape(campo)}\\\\s*', '. ', definicion_parte)\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion_parte = re.sub(r'SINÓN:.*$', '', definicion_parte)\n",
    "        definicion_parte = re.sub(r'EJEM:.*$', '', definicion_parte)\n",
    "        definicion_parte = re.sub(r'VARIEDADES:.*$', '', definicion_parte, flags=re.IGNORECASE)\n",
    "        definicion_parte = re.sub(r'^\\([^)]*\\)\\.\\s*', '', definicion_parte)  # Nombres científicos solos\n",
    "        \n",
    "        # REGLA 3: Evitar contaminación - detectar nueva entrada\n",
    "        patron_nueva_entrada = r'\\b[a-zA-Z]+\\.\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.|\\bV\\.)'\n",
    "        match_nueva = re.search(patron_nueva_entrada, definicion_parte)\n",
    "        if match_nueva:\n",
    "            definicion_parte = definicion_parte[:match_nueva.start()].strip()\n",
    "        \n",
    "        definicion_parte = re.sub(r'\\s+', ' ', definicion_parte).strip(' .,!')\n",
    "        \n",
    "        # AGREGAR A DEFINICIONES TOTALES\n",
    "        # Primera parte siempre se incluye, partes después de || se incluyen si no son puramente dialectales\n",
    "        if i == 0:\n",
    "            # Primera parte - siempre incluir\n",
    "            if definicion_parte:\n",
    "                todas_definiciones.append(definicion_parte)\n",
    "        else:\n",
    "            # Partes después de || - incluir si hay contenido y no es solo dialectal\n",
    "            if definicion_parte and not es_dialectal:\n",
    "                todas_definiciones.append(definicion_parte)\n",
    "            elif definicion_parte and es_dialectal:\n",
    "                # Si es dialectal pero hay contenido antes del dialecto, incluirlo\n",
    "                texto_antes_dialectos = definicion_parte\n",
    "                for dialecto in dialectales:\n",
    "                    dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                    patron_corte = f'{dialecto_sin_punto}:'\n",
    "                    if patron_corte in texto_antes_dialectos:\n",
    "                        pos_corte = texto_antes_dialectos.find(patron_corte)\n",
    "                        texto_antes_dialectos = texto_antes_dialectos[:pos_corte].strip()\n",
    "                        break\n",
    "                if texto_antes_dialectos:\n",
    "                    todas_definiciones.append(texto_antes_dialectos)\n",
    "        \n",
    "    # COMBINAR TODAS LAS DEFINICIONES\n",
    "    if todas_definiciones:\n",
    "        entrada['definicion'] = '. '.join(todas_definiciones)\n",
    "    \n",
    "    # FORMATEAR CATEGORÍA GRAMATICAL\n",
    "    if len(entrada['categoria_gramatical']) == 1:\n",
    "        entrada['categoria_gramatical'] = entrada['categoria_gramatical'][0]\n",
    "    \n",
    "    # FORMATEAR CAMPO SEMÁNTICO - mantener como lista vacía si no hay campos\n",
    "    if not entrada['campo_semantico']:\n",
    "        entrada['campo_semantico'] = []\n",
    "    \n",
    "    # RETORNAR ENTRADA ÚNICA (ya no necesitamos múltiples entradas por lema)\n",
    "    if (entrada['definicion'] or entrada['sinonimos'] or \n",
    "        entrada['variantes_dialectales'] or entrada.get('remite_a')):\n",
    "        return [entrada]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "class QuechuaEspanolParser:\n",
    "    \"\"\"\n",
    "    Parser específico para la sección Quechua-Español\n",
    "    Utiliza el parser ultra-estricto que separa correctamente los campos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, abreviaturas: Dict[str, List[str]]):\n",
    "        self.abreviaturas = abreviaturas\n",
    "    \n",
    "    def parsear_seccion(self, texto: str, max_entradas: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parsea la sección Quechua-Español con el parser ultra-estricto\n",
    "        \"\"\"\n",
    "        entradas = []\n",
    "        \n",
    "        # Extraer entradas completas (multilinea)\n",
    "        entradas_texto = extraer_entradas_completas(texto)\n",
    "        print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "        \n",
    "        contador = 0\n",
    "        for entrada_texto in entradas_texto:\n",
    "            if contador >= max_entradas:\n",
    "                break\n",
    "                \n",
    "            entradas_procesadas = parsear_entrada_ultra_estricta(entrada_texto, self.abreviaturas, es_espanol=False)\n",
    "            if entradas_procesadas:\n",
    "                entradas.extend(entradas_procesadas)\n",
    "                contador += len(entradas_procesadas)\n",
    "        \n",
    "        return entradas\n",
    "\n",
    "class EspanolQuechuaParser:\n",
    "    \"\"\"\n",
    "    Parser específico para la sección Español-Quechua\n",
    "    Utiliza el parser ultra-estricto que separa correctamente los campos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, abreviaturas: Dict[str, List[str]]):\n",
    "        self.abreviaturas = abreviaturas\n",
    "    \n",
    "    def parsear_seccion(self, texto: str, max_entradas: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parsea la sección Español-Quechua con el parser ultra-estricto\n",
    "        \"\"\"\n",
    "        entradas = []\n",
    "        \n",
    "        # Extraer entradas completas (multilinea)\n",
    "        entradas_texto = extraer_entradas_completas(texto)\n",
    "        print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "        \n",
    "        contador = 0\n",
    "        for entrada_texto in entradas_texto:\n",
    "            if contador >= max_entradas:\n",
    "                break\n",
    "                \n",
    "            entradas_procesadas = parsear_entrada_ultra_estricta(entrada_texto, self.abreviaturas, es_espanol=True)\n",
    "            if entradas_procesadas:\n",
    "                entradas.extend(entradas_procesadas)\n",
    "                contador += len(entradas_procesadas)\n",
    "        \n",
    "        return entradas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44784e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando sección Quechua-Español...\n",
      "Encontradas 16277 entradas potenciales\n",
      "Encontradas 16277 entradas potenciales\n",
      "9978 entradas procesadas\n",
      "\n",
      "Procesando sección Español-Quechua...\n",
      "Encontradas 4577 entradas potenciales\n",
      "9978 entradas procesadas\n",
      "\n",
      "Procesando sección Español-Quechua...\n",
      "Encontradas 4577 entradas potenciales\n",
      "4571 entradas procesadas\n",
      "4571 entradas procesadas\n",
      "\n",
      "Guardado: quechua_espanol.json (9978 entradas)\n",
      "\n",
      "Guardado: quechua_espanol.json (9978 entradas)\n",
      "Guardado: espanol_quechua.json (4571 entradas)\n",
      "Guardado: espanol_quechua.json (4571 entradas)\n"
     ]
    }
   ],
   "source": [
    "# TAREA 5: Generación de archivos JSON estructurados\n",
    "\n",
    "import time\n",
    "\n",
    "def cargar_abreviaturas():\n",
    "    \"\"\"\n",
    "    Carga las abreviaturas desde el archivo JSON generado anteriormente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"abreviaturas.json\", 'r', encoding='utf-8') as f:\n",
    "            abreviaturas = json.load(f)\n",
    "        return abreviaturas\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: archivo abreviaturas.json no encontrado\")\n",
    "        return {\n",
    "            'categorias_gramaticales': ['s.', 'adj.', 'v.', 'adv.', 'interj.', 'prep.', 'conj.', 'pron.'],\n",
    "            'campos_semanticos': ['Bot.', 'Zool.', 'Med.', 'Hist.', 'Geog.'],\n",
    "            'dialectales': ['Arg.', 'Bol.', 'Pe.Anc.', 'Pe.Aya.', 'Pe.Qos.']\n",
    "        }\n",
    "\n",
    "def generar_archivos_json():\n",
    "    \"\"\"\n",
    "    Ejecuta los parsers y genera los archivos JSON según las especificaciones\n",
    "    \"\"\"\n",
    "    # Cargar abreviaturas\n",
    "    abreviaturas = cargar_abreviaturas()\n",
    "\n",
    "    # Crear parsers usando las clases de la Tarea 4\n",
    "    parser_qe = QuechuaEspanolParser(abreviaturas)\n",
    "    parser_eq = EspanolQuechuaParser(abreviaturas)\n",
    "    \n",
    "    # Procesar sección Quechua-Español\n",
    "    entradas_qe = []\n",
    "    if os.path.exists(\"seccion_quechua_espanol.txt\"):\n",
    "        print(\"\\nProcesando sección Quechua-Español...\")\n",
    "        with open(\"seccion_quechua_espanol.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_qe = f.read()\n",
    "        \n",
    "        entradas_qe = parser_qe.parsear_seccion(texto_qe, max_entradas=15000)\n",
    "        print(f\"{len(entradas_qe)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"Archivo seccion_quechua_espanol.txt no encontrado\")\n",
    "    \n",
    "    # Procesar sección Español-Quechua\n",
    "    entradas_eq = []\n",
    "    if os.path.exists(\"seccion_espanol_quechua.txt\"):\n",
    "        print(\"\\nProcesando sección Español-Quechua...\")\n",
    "        with open(\"seccion_espanol_quechua.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_eq = f.read()\n",
    "        \n",
    "        entradas_eq = parser_eq.parsear_seccion(texto_eq, max_entradas=15000)\n",
    "        print(f\"{len(entradas_eq)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"Archivo seccion_espanol_quechua.txt no encontrado\")\n",
    "    \n",
    "    # Guardar archivos JSON\n",
    "    if entradas_qe:\n",
    "        with open(\"quechua_espanol.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_qe, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nGuardado: quechua_espanol.json ({len(entradas_qe)} entradas)\")\n",
    "    \n",
    "    if entradas_eq:\n",
    "        with open(\"espanol_quechua.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_eq, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Guardado: espanol_quechua.json ({len(entradas_eq)} entradas)\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "generar_archivos_json()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
