{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f7bcf4",
   "metadata": {},
   "source": [
    "# CONSTRUCCI√ìN DE RECURSOS L√âXICOS PARA QUECHUA\n",
    "\n",
    "## Objetivo\n",
    "Extraer, estructurar y enriquecer el diccionario biling√ºe Quechua-Espa√±ol para crear un corpus l√©xico machine-readable.\n",
    "\n",
    "## Estructura del Proyecto\n",
    "- **Entrada**: Diccionario PDF biling√ºe (Quechua-Espa√±ol / Espa√±ol-Quechua)\n",
    "- **Salida**: Archivos JSON estructurados + librer√≠a Python para consultas\n",
    "\n",
    "## Tareas\n",
    "1. Extracci√≥n de texto crudo del PDF\n",
    "2. Identificaci√≥n y separaci√≥n de secciones\n",
    "3. Dise√±o e implementaci√≥n de parsers\n",
    "4. Generaci√≥n de archivos JSON estructurados\n",
    "5. Desarrollo de librer√≠a de utilidades\n",
    "6. Validaci√≥n y testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dfd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librer√≠as instaladas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de librer√≠as necesarias\n",
    "# !pip install PyMuPDF pdfplumber regex\n",
    "\n",
    "# Importaciones\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Librer√≠as instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940bb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo texto del PDF...\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extra√≠do: 1930001 caracteres\n",
      "Texto crudo guardado en: diccionario_raw.txt\n",
      "Texto extra√≠do: 1930001 caracteres\n"
     ]
    }
   ],
   "source": [
    "# TAREA 1 y 2: Extracci√≥n de texto del PDF y separaci√≥n de secciones\n",
    "\n",
    "def extraer_texto_pdf(ruta_pdf: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae texto crudo del PDF preservando estructura b√°sica sin alteraciones\n",
    "    \"\"\"\n",
    "    texto_completo = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Usar PyMuPDF para extracci√≥n\n",
    "        doc = fitz.open(ruta_pdf)\n",
    "        \n",
    "        for pagina_num in range(doc.page_count):\n",
    "            pagina = doc[pagina_num]\n",
    "            texto = pagina.get_text()\n",
    "            texto_completo += texto\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error con PyMuPDF: {e}\")\n",
    "        \n",
    "        # Alternativa con pdfplumber\n",
    "        try:\n",
    "            with pdfplumber.open(ruta_pdf) as pdf:\n",
    "                for pagina in pdf.pages:\n",
    "                    texto = pagina.extract_text()\n",
    "                    if texto:\n",
    "                        texto_completo += texto\n",
    "        except Exception as e2:\n",
    "            print(f\"Error con pdfplumber: {e2}\")\n",
    "            return \"\"\n",
    "    \n",
    "    return texto_completo\n",
    "\n",
    "def guardar_texto_crudo(texto: str, ruta_salida: str = \"diccionario_raw.txt\"):\n",
    "    \"\"\"\n",
    "    Guarda el texto crudo extra√≠do del PDF\n",
    "    \"\"\"\n",
    "    with open(ruta_salida, 'w', encoding='utf-8') as f:\n",
    "        f.write(texto)\n",
    "    print(f\"Texto crudo guardado en: {ruta_salida}\")\n",
    "\n",
    "# Ejecutar extracci√≥n\n",
    "ruta_pdf = \"diccionario-qeswa-academia-mayor.pdf\"\n",
    "if os.path.exists(ruta_pdf):\n",
    "    print(\"Extrayendo texto del PDF...\")\n",
    "    texto_crudo = extraer_texto_pdf(ruta_pdf)\n",
    "    guardar_texto_crudo(texto_crudo)\n",
    "    print(f\"Texto extra√≠do: {len(texto_crudo)} caracteres\")\n",
    "else:\n",
    "    print(f\"Archivo PDF no encontrado: {ruta_pdf}\")\n",
    "    print(\"Por favor, aseg√∫rese de que el archivo est√© en el directorio correcto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d73b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo abreviaturas...\n",
      "Dialectales: 18\n",
      "Categor√≠as gramaticales: 46\n",
      "Campos sem√°nticos: 93\n",
      "\n",
      "Separando secciones...\n",
      "Secci√≥n Quechua-Espa√±ol: 1615928 caracteres\n",
      "Secci√≥n Espa√±ol-Quechua: 277475 caracteres\n"
     ]
    }
   ],
   "source": [
    "# TAREA 3: Identificaci√≥n de secciones y extracci√≥n autom√°tica de abreviaturas\n",
    "\n",
    "def extraer_abreviaturas_automaticamente(texto: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extrae autom√°ticamente abreviaturas dialectales, categor√≠as gramaticales y campos sem√°nticos\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    \n",
    "    abreviaturas = {\n",
    "        'categorias_gramaticales': [],\n",
    "        'campos_semanticos': [],\n",
    "        'dialectales': []\n",
    "    }\n",
    "    \n",
    "    # 1. EXTRAER ABREVIATURAS DIALECTALES\n",
    "    dialectales_extraidas = set()\n",
    "    \n",
    "    for i in range(792, min(853, len(lineas))):\n",
    "        linea = lineas[i].strip()\n",
    "        \n",
    "        # Pa√≠ses: \"1. Arg.\"\n",
    "        match_pais = re.match(r'^\\d+\\.\\s+([A-Z][a-z]*\\.)\\s*$', linea)\n",
    "        if match_pais:\n",
    "            dialectales_extraidas.add(match_pais.group(1))\n",
    "        \n",
    "        # Regiones peruanas: \"1. Pe.Anc.\"\n",
    "        match_region = re.match(r'^\\d+\\.\\s+(Pe\\.[A-Za-z]+\\.)\\s*$', linea)\n",
    "        if match_region:\n",
    "            dialectales_extraidas.add(match_region.group(1))\n",
    "    \n",
    "    abreviaturas['dialectales'] = sorted(list(dialectales_extraidas))\n",
    "    \n",
    "    # 2. EXTRAER ABREVIATURAS DE CATEGOR√çAS Y CAMPOS SEM√ÅNTICOS\n",
    "    inicio_abreviaturas = 853\n",
    "    \n",
    "    # Buscar fin de secci√≥n\n",
    "    fin_abreviaturas = 1320  # Default\n",
    "    for i in range(1000, min(1400, len(lineas))):\n",
    "        linea = lineas[i].strip().upper()\n",
    "        if \"AUTORES\" in linea and \"CONSULTADOS\" in linea:\n",
    "            fin_abreviaturas = i\n",
    "            break\n",
    "    \n",
    "    # Extraer abreviaturas\n",
    "    todas_abreviaturas = set()\n",
    "    \n",
    "    for i in range(inicio_abreviaturas, fin_abreviaturas):\n",
    "        if i < len(lineas):\n",
    "            linea = lineas[i].strip()\n",
    "            \n",
    "            if (linea and linea.endswith('.') and len(linea) <= 25 and \n",
    "                ' ' not in linea and len(linea) >= 2 and\n",
    "                not re.match(r'^\\d+\\.?$', linea) and \n",
    "                not re.match(r'^[^a-zA-Z]+\\.$', linea)):\n",
    "                todas_abreviaturas.add(linea)\n",
    "    \n",
    "    # 3. CLASIFICAR ABREVIATURAS\n",
    "    categorias_gramaticales = [\n",
    "        'adj.', 'adv.', 's.', 'v.', 'interj.', 'prep.', 'conj.', 'pron.',\n",
    "        'm.', 'f.', 'pl.', 'sing.', 'loc.', 'loc.adv.', 'n√∫m.', 'n√∫m.card.',\n",
    "        'n√∫m.ord.', 'imper.', 'inf√≠nit.', 'interrog.', 'negat.', 'gen.',\n",
    "        'alfab.', 'diminut.', 'ant√≥n.', 'sin√≥n.', 'par√≥n.', 'ap√≥c.',\n",
    "        'etim.', 'onomat.', 'neol.', 'figdo.', 'fam.',\n",
    "        'ejem.', 'dep.', 'dist.', 'prov.', 'bibliogr.', 'calend.',\n",
    "        'comer.', 'medid.', 'pref.', 'suf.', 'tej.', 'S.', 'alim.'\n",
    "    ]\n",
    "    \n",
    "    categorias_encontradas = set()\n",
    "    campos_semanticos = set()\n",
    "    \n",
    "    for abrev in todas_abreviaturas:\n",
    "        if abrev in categorias_gramaticales:\n",
    "            categorias_encontradas.add(abrev)\n",
    "        else:\n",
    "            campos_semanticos.add(abrev)\n",
    "    \n",
    "    abreviaturas['categorias_gramaticales'] = sorted(list(categorias_encontradas))\n",
    "    abreviaturas['campos_semanticos'] = sorted(list(campos_semanticos))\n",
    "    \n",
    "    return abreviaturas\n",
    "\n",
    "def separar_secciones_por_lineas(texto: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Separa las secciones usando los n√∫meros de l√≠nea conocidos\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    \n",
    "    # Secci√≥n Quechua-Espa√±ol: l√≠nea 1325 a 50998\n",
    "    seccion_qe_lineas = lineas[1325:50998]\n",
    "    \n",
    "    # Limpiar texto extra al final de la secci√≥n Quechua-Espa√±ol\n",
    "    seccion_qe_limpia = []\n",
    "    for linea in seccion_qe_lineas:\n",
    "        if \"ESPA√ëOL - QUECHUA\" in linea:\n",
    "            break\n",
    "        seccion_qe_limpia.append(linea)\n",
    "    \n",
    "    # Secci√≥n Espa√±ol-Quechua: l√≠nea 50998 hasta el final\n",
    "    seccion_eq_lineas = []\n",
    "    for i in range(50998, len(lineas)):\n",
    "        linea = lineas[i].strip()\n",
    "        if (\"DICCIONARIO QUECHUA ‚Äì ESPA√ëOL ‚Äì QUECHUA\" in linea or \n",
    "            \"se termin√≥ de imprimir\" in linea or \n",
    "            \"talleres gr√°ficos\" in linea):\n",
    "            break\n",
    "        seccion_eq_lineas.append(lineas[i])\n",
    "    \n",
    "    return '\\n'.join(seccion_qe_limpia), '\\n'.join(seccion_eq_lineas)\n",
    "\n",
    "# Ejecutar extracci√≥n\n",
    "if os.path.exists(\"diccionario_raw.txt\"):\n",
    "    with open(\"diccionario_raw.txt\", 'r', encoding='utf-8') as f:\n",
    "        texto_completo = f.read()\n",
    "    \n",
    "    print(\"Extrayendo abreviaturas...\")\n",
    "    abreviaturas = extraer_abreviaturas_automaticamente(texto_completo)\n",
    "    \n",
    "    print(f\"Dialectales: {len(abreviaturas['dialectales'])}\")\n",
    "    print(f\"Categor√≠as gramaticales: {len(abreviaturas['categorias_gramaticales'])}\")\n",
    "    print(f\"Campos sem√°nticos: {len(abreviaturas['campos_semanticos'])}\")\n",
    "    \n",
    "    print(\"\\nSeparando secciones...\")\n",
    "    seccion_qe, seccion_eq = separar_secciones_por_lineas(texto_completo)\n",
    "    \n",
    "    print(f\"Secci√≥n Quechua-Espa√±ol: {len(seccion_qe)} caracteres\")\n",
    "    print(f\"Secci√≥n Espa√±ol-Quechua: {len(seccion_eq)} caracteres\")\n",
    "    \n",
    "    # Guardar archivos\n",
    "    with open(\"seccion_quechua_espanol.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_qe)\n",
    "    \n",
    "    with open(\"seccion_espanol_quechua.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(seccion_eq)\n",
    "    \n",
    "    with open(\"abreviaturas.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(abreviaturas, f, indent=2, ensure_ascii=False)\n",
    "else:\n",
    "    print(\"Archivo diccionario_raw.txt no encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcdafa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsers ultra-estrictos implementados correctamente\n",
      "‚úÖ Reglas de separaci√≥n de campos aplicadas seg√∫n especificaciones\n"
     ]
    }
   ],
   "source": [
    "# TAREA 4: Dise√±o e implementaci√≥n de parsers\n",
    "\n",
    "def extraer_entradas_completas(texto: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrae entradas completas del texto, incluyendo las que abarcan m√∫ltiples l√≠neas\n",
    "    \"\"\"\n",
    "    lineas = texto.split('\\n')\n",
    "    entradas = []\n",
    "    entrada_actual = \"\"\n",
    "    \n",
    "    for linea in lineas:\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Verificar si es el inicio de una nueva entrada\n",
    "        # Patr√≥n: palabra seguida de punto y categor√≠a gramatical\n",
    "        if re.match(r'^[a-zA-Z√Ä-√ø√±√ë!¬°¬ø?]+[\\.!]\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.)', linea):\n",
    "            # Guardar entrada anterior si existe\n",
    "            if entrada_actual.strip():\n",
    "                entradas.append(entrada_actual.strip())\n",
    "            # Iniciar nueva entrada\n",
    "            entrada_actual = linea\n",
    "        else:\n",
    "            # Continuar entrada actual\n",
    "            if entrada_actual:\n",
    "                entrada_actual += \" \" + linea\n",
    "    \n",
    "    # Agregar la √∫ltima entrada\n",
    "    if entrada_actual.strip():\n",
    "        entradas.append(entrada_actual.strip())\n",
    "    \n",
    "    return entradas\n",
    "\n",
    "def parsear_entrada_ultra_estricta(entrada_texto: str, abreviaturas: Dict, es_espanol: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parser ULTRA-ESTRICTO que separa correctamente definici√≥n de variantes dialectales.\n",
    "    Implementa exactamente las reglas especificadas por el usuario.\n",
    "    \"\"\"\n",
    "    if len(entrada_texto) < 10:\n",
    "        return []\n",
    "    \n",
    "    # 1. EXTRAER LEMA (antes del primer punto o exclamaci√≥n)\n",
    "    match_lema = re.match(r'^([a-zA-Z√Ä-√ø√±√ë!¬°¬ø?,\\s]+?)[\\.!]\\s', entrada_texto)\n",
    "    if not match_lema:\n",
    "        return []\n",
    "    \n",
    "    lema = match_lema.group(1).strip()\n",
    "    # Min√∫scula salvo nombres propios\n",
    "    if not lema[0].isupper():\n",
    "        lema = lema.lower()\n",
    "    \n",
    "    # Extraer resto del texto despu√©s del lema\n",
    "    resto = entrada_texto[len(match_lema.group(0)):].strip()\n",
    "    \n",
    "    # Filtro para espa√±ol: evitar palabras quechuas\n",
    "    if es_espanol:\n",
    "        if any(c in lema.lower() for c in ['k', 'w', 'q']) and 'qu' not in lema.lower():\n",
    "            return []\n",
    "    \n",
    "    # Obtener listas de abreviaturas\n",
    "    categorias_gramaticales = abreviaturas.get('categorias_gramaticales', [])\n",
    "    campos_semanticos = abreviaturas.get('campos_semanticos', [])\n",
    "    dialectales = abreviaturas.get('dialectales', [])\n",
    "    \n",
    "    # 2. EXTRAER CATEGOR√çA GRAMATICAL (inmediatamente despu√©s del lema)\n",
    "    categoria = ''\n",
    "    for cat in sorted(categorias_gramaticales, key=len, reverse=True):\n",
    "        if resto.startswith(cat):\n",
    "            categoria = cat\n",
    "            resto = resto[len(cat):].strip()\n",
    "            break\n",
    "    \n",
    "    if not categoria:\n",
    "        return []\n",
    "    \n",
    "    # 3. EXTRAER CAMPO SEM√ÅNTICO (si aparece despu√©s de la categor√≠a)\n",
    "    campo_semantico = ''\n",
    "    for campo in sorted(campos_semanticos, key=len, reverse=True):\n",
    "        if resto.startswith(campo):\n",
    "            campo_semantico = campo\n",
    "            resto = resto[len(campo):].strip()\n",
    "            break\n",
    "    \n",
    "    # 4. SEPARAR M√öLTIPLES ACEPCIONES POR ||\n",
    "    acepciones = [a.strip() for a in resto.split('||') if a.strip()]\n",
    "    \n",
    "    entradas_resultado = []\n",
    "    \n",
    "    for acepcion in acepciones:\n",
    "        entrada = {\n",
    "            'lema': lema,\n",
    "            'categoria_gramatical': categoria,\n",
    "            'campo_semantico': campo_semantico,\n",
    "            'definicion': '',\n",
    "            'variantes_dialectales': {},\n",
    "            'sinonimos': [],\n",
    "            'ejemplos': []\n",
    "        }\n",
    "        \n",
    "        texto = acepcion.strip()\n",
    "        \n",
    "        # 5. DETECTAR SI HAY UNA NUEVA ENTRADA EN EL TEXTO (problema cr√≠tico)\n",
    "        # Patr√≥n: palabra seguida de punto y categor√≠a gramatical = nueva entrada\n",
    "        nueva_entrada_match = re.search(r'\\b([a-zA-Z√Ä-√ø√±√ë!¬°¬ø?]+[\\.!])\\s+(s\\.|adj\\.|v\\.|adv\\.|interj\\.|prep\\.|conj\\.|pron\\.|alfab\\.|loc\\.)', texto)\n",
    "        if nueva_entrada_match:\n",
    "            # Hay una nueva entrada mezclada - cortar aqu√≠\n",
    "            pos_nueva_entrada = nueva_entrada_match.start()\n",
    "            texto = texto[:pos_nueva_entrada].strip()\n",
    "        \n",
    "        # 6. EXTRAER EJEMPLOS PRIMERO (EJEM:)\n",
    "        if 'EJEM:' in texto:\n",
    "            pos_ejem = texto.find('EJEM:')\n",
    "            texto_antes = texto[:pos_ejem].strip()\n",
    "            texto_ejem = texto[pos_ejem + 5:].strip()\n",
    "            entrada['ejemplos'] = [texto_ejem.strip(' .!')]\n",
    "            texto = texto_antes\n",
    "        \n",
    "        # 7. EXTRAER SIN√ìNIMOS (SIN√ìN:) \n",
    "        if 'SIN√ìN:' in texto:\n",
    "            pos_sinon = texto.find('SIN√ìN:')\n",
    "            texto_antes = texto[:pos_sinon].strip()\n",
    "            texto_sinon = texto[pos_sinon + 6:].strip()\n",
    "            \n",
    "            # REGLA CR√çTICA: Los sin√≥nimos terminan en el PRIMER punto seguido de may√∫scula\n",
    "            # O cuando aparece una abreviatura dialectal\n",
    "            fin_sinon = len(texto_sinon)\n",
    "            \n",
    "            # Buscar primer punto seguido de may√∫scula\n",
    "            for i, char in enumerate(texto_sinon):\n",
    "                if char == '.' and i + 1 < len(texto_sinon):\n",
    "                    siguiente = texto_sinon[i + 1:i + 10].strip()\n",
    "                    if siguiente and siguiente[0].isupper():\n",
    "                        fin_sinon = i + 1\n",
    "                        break\n",
    "            \n",
    "            # Buscar abreviaturas dialectales que interrumpen sin√≥nimos\n",
    "            for dialecto in dialectales:\n",
    "                dialecto_sin_punto = dialecto.rstrip('.')\n",
    "                for patron in [f' {dialecto_sin_punto}:', f' {dialecto_sin_punto} ']:\n",
    "                    pos = texto_sinon.find(patron)\n",
    "                    if pos != -1 and pos < fin_sinon:\n",
    "                        fin_sinon = pos\n",
    "            \n",
    "            sinon_texto = texto_sinon[:fin_sinon].strip(' .,!')\n",
    "            if sinon_texto:\n",
    "                sinonimos = [s.strip() for s in sinon_texto.split(',') if s.strip()]\n",
    "                # Limpiar sin√≥nimos de caracteres extra\n",
    "                entrada['sinonimos'] = [re.sub(r'[^\\w\\s√±√ë√°√©√≠√≥√∫√Å√â√ç√ì√ö]', '', s).strip() \n",
    "                                      for s in sinonimos if len(s.strip()) > 1]\n",
    "            \n",
    "            # Continuar con el resto despu√©s de los sin√≥nimos\n",
    "            resto_despues_sinon = texto_sinon[fin_sinon:].strip()\n",
    "            texto = texto_antes + ' ' + resto_despues_sinon\n",
    "            texto = texto.strip()\n",
    "        \n",
    "        # 8. EXTRAER VARIANTES DIALECTALES\n",
    "        # REGLA CR√çTICA: Pe.Aya:, Arg:, Bol:, etc. indican variantes dialectales\n",
    "        texto_limpio = texto\n",
    "        \n",
    "        # Ordenar dialectales por longitud (m√°s largos primero) para evitar conflictos\n",
    "        dialectales_ordenados = sorted(dialectales, key=len, reverse=True)\n",
    "        \n",
    "        for dialecto in dialectales_ordenados:\n",
    "            # Buscar patr√≥n: espacio + dialecto + dos puntos (ej: \" Pe.Aya:\", \" Arg:\")\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            patron_dialecto = f' {dialecto_sin_punto}:'\n",
    "            \n",
    "            if patron_dialecto in texto_limpio:\n",
    "                pos_inicio = texto_limpio.find(patron_dialecto)\n",
    "                texto_antes = texto_limpio[:pos_inicio].strip()\n",
    "                texto_despues = texto_limpio[pos_inicio + len(patron_dialecto):].strip()\n",
    "                \n",
    "                # Encontrar final de las variantes dialectales\n",
    "                fin_dialectal = len(texto_despues)\n",
    "                \n",
    "                # Buscar el pr√≥ximo dialecto (m√°s espec√≠fico primero)\n",
    "                for otro_dialecto in dialectales_ordenados:\n",
    "                    if otro_dialecto != dialecto:\n",
    "                        otro_sin_punto = otro_dialecto.rstrip('.')\n",
    "                        patron_otro = f' {otro_sin_punto}:'\n",
    "                        pos_otro = texto_despues.find(patron_otro)\n",
    "                        if pos_otro != -1 and pos_otro < fin_dialectal:\n",
    "                            fin_dialectal = pos_otro\n",
    "                \n",
    "                # Buscar marcadores que interrumpen dialectales\n",
    "                for marcador in [' SIN√ìN:', ' EJEM:', '. SIN√ìN:', '. EJEM:']:\n",
    "                    pos_marcador = texto_despues.find(marcador)\n",
    "                    if pos_marcador != -1 and pos_marcador < fin_dialectal:\n",
    "                        fin_dialectal = pos_marcador\n",
    "                \n",
    "                # Buscar punto seguido de may√∫scula (nueva entrada)\n",
    "                for i, char in enumerate(texto_despues):\n",
    "                    if char == '.' and i + 1 < len(texto_despues):\n",
    "                        siguiente = texto_despues[i + 1:i + 10].strip()\n",
    "                        if siguiente and siguiente[0].isupper():\n",
    "                            fin_dialectal = i + 1\n",
    "                            break\n",
    "                \n",
    "                variante_texto = texto_despues[:fin_dialectal].strip(' .,!')\n",
    "                if variante_texto:\n",
    "                    # Determinar pa√≠s y regi√≥n para estructura anidada\n",
    "                    if '.' in dialecto and len(dialecto.split('.', 1)[1].rstrip('.')) > 0:\n",
    "                        # Caso: Pe.Aya. ‚Üí pais='Pe', region='Aya'\n",
    "                        pais, region = dialecto.split('.', 1)\n",
    "                        region = region.rstrip('.')\n",
    "                    else:\n",
    "                        # Caso: Arg. ‚Üí pais='Arg', region='Gen'\n",
    "                        pais = dialecto.rstrip('.')\n",
    "                        region = 'Gen'\n",
    "                    \n",
    "                    # Procesar variantes (separadas por comas)\n",
    "                    variantes = [v.strip() for v in variante_texto.split(',') if v.strip()]\n",
    "                    \n",
    "                    # Crear estructura anidada\n",
    "                    if pais not in entrada['variantes_dialectales']:\n",
    "                        entrada['variantes_dialectales'][pais] = {}\n",
    "                    entrada['variantes_dialectales'][pais][region] = variantes\n",
    "                \n",
    "                # Limpiar del texto principal\n",
    "                texto_limpio = texto_antes + ' ' + texto_despues[fin_dialectal:]\n",
    "                texto_limpio = texto_limpio.strip()\n",
    "        \n",
    "        # 9. DEFINICI√ìN (lo que queda despu√©s de limpiar todo)\n",
    "        definicion = texto_limpio\n",
    "        \n",
    "        # Limpiar marcadores residuales\n",
    "        definicion = re.sub(r'SIN√ìN:.*$', '', definicion)\n",
    "        definicion = re.sub(r'EJEM:.*$', '', definicion)\n",
    "        \n",
    "        # Limpiar dialectales residuales\n",
    "        for dialecto in dialectales:\n",
    "            dialecto_sin_punto = dialecto.rstrip('.')\n",
    "            definicion = re.sub(f'{re.escape(dialecto_sin_punto)}:.*$', '', definicion)\n",
    "        \n",
    "        # REGLA CR√çTICA: La definici√≥n termina en el primer punto seguido de may√∫scula\n",
    "        match_fin_def = re.search(r'([^.]*\\.)(?=\\s*[A-Z])', definicion)\n",
    "        if match_fin_def:\n",
    "            definicion = match_fin_def.group(1)\n",
    "        \n",
    "        # Normalizar espacios y puntuaci√≥n\n",
    "        definicion = re.sub(r'\\s+', ' ', definicion).strip(' .,!')\n",
    "        \n",
    "        if definicion:\n",
    "            entrada['definicion'] = definicion\n",
    "        \n",
    "        # 10. Para espa√±ol-quechua, extraer traducciones\n",
    "        if es_espanol and not entrada['definicion'] and acepcion:\n",
    "            palabras = acepcion.split()[:3]\n",
    "            traducciones = [p.strip('.,;!') for p in palabras if len(p) > 2]\n",
    "            if traducciones:\n",
    "                entrada['traducciones_quechua'] = traducciones\n",
    "                entrada['definicion'] = ', '.join(traducciones)\n",
    "        \n",
    "        # Solo agregar si tiene contenido v√°lido\n",
    "        if entrada['definicion'] or entrada['sinonimos'] or entrada['variantes_dialectales']:\n",
    "            entradas_resultado.append(entrada)\n",
    "    \n",
    "    return entradas_resultado\n",
    "\n",
    "class QuechuaEspanolParser:\n",
    "    \"\"\"\n",
    "    Parser espec√≠fico para la secci√≥n Quechua-Espa√±ol\n",
    "    Utiliza el parser ultra-estricto que separa correctamente los campos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, abreviaturas: Dict[str, List[str]]):\n",
    "        self.abreviaturas = abreviaturas\n",
    "    \n",
    "    def parsear_seccion(self, texto: str, max_entradas: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parsea la secci√≥n Quechua-Espa√±ol con el parser ultra-estricto\n",
    "        \"\"\"\n",
    "        entradas = []\n",
    "        \n",
    "        # Extraer entradas completas (multilinea)\n",
    "        entradas_texto = extraer_entradas_completas(texto)\n",
    "        print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "        \n",
    "        contador = 0\n",
    "        for entrada_texto in entradas_texto:\n",
    "            if contador >= max_entradas:\n",
    "                break\n",
    "                \n",
    "            entradas_procesadas = parsear_entrada_ultra_estricta(entrada_texto, self.abreviaturas, es_espanol=False)\n",
    "            if entradas_procesadas:\n",
    "                entradas.extend(entradas_procesadas)\n",
    "                contador += len(entradas_procesadas)\n",
    "        \n",
    "        return entradas\n",
    "\n",
    "class EspanolQuechuaParser:\n",
    "    \"\"\"\n",
    "    Parser espec√≠fico para la secci√≥n Espa√±ol-Quechua\n",
    "    Utiliza el parser ultra-estricto que separa correctamente los campos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, abreviaturas: Dict[str, List[str]]):\n",
    "        self.abreviaturas = abreviaturas\n",
    "    \n",
    "    def parsear_seccion(self, texto: str, max_entradas: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Parsea la secci√≥n Espa√±ol-Quechua con el parser ultra-estricto\n",
    "        \"\"\"\n",
    "        entradas = []\n",
    "        \n",
    "        # Extraer entradas completas (multilinea)\n",
    "        entradas_texto = extraer_entradas_completas(texto)\n",
    "        print(f\"Encontradas {len(entradas_texto)} entradas potenciales\")\n",
    "        \n",
    "        contador = 0\n",
    "        for entrada_texto in entradas_texto:\n",
    "            if contador >= max_entradas:\n",
    "                break\n",
    "                \n",
    "            entradas_procesadas = parsear_entrada_ultra_estricta(entrada_texto, self.abreviaturas, es_espanol=True)\n",
    "            if entradas_procesadas:\n",
    "                entradas.extend(entradas_procesadas)\n",
    "                contador += len(entradas_procesadas)\n",
    "        \n",
    "        return entradas\n",
    "\n",
    "print(\"‚úÖ Parsers ultra-estrictos implementados correctamente\")\n",
    "print(\"‚úÖ Reglas de separaci√≥n de campos aplicadas seg√∫n especificaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f44784e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando generaci√≥n completa...\n",
      "\n",
      "=== TEST DE CASOS PROBLEM√ÅTICOS ===\n",
      "\n",
      "üß™ Caso de prueba 1:\n",
      "Entrada: achacha. s. Juguete. SIN√ìN: pukllana. Pe.Aya: pujllana. Arg: achala, achocha. Bo...\n",
      "   Acepci√≥n 1:\n",
      "     üè∑Ô∏è  Lema: 'achacha'\n",
      "     üìù Definici√≥n: 'Juguete'\n",
      "     üîÑ Sin√≥nimos: ['pukllana']\n",
      "     üåç Dialectales: {'Pe': {'Aya': ['pujllana']}, 'Arg': {'Gen': ['achala', 'achocha']}, 'Bol': {'Gen': ['pukllana', 'phukllana']}}\n",
      "   Acepci√≥n 2:\n",
      "     üè∑Ô∏è  Lema: 'achacha'\n",
      "     üìù Definici√≥n: 'Vestido lujoso'\n",
      "     üîÑ Sin√≥nimos: []\n",
      "     üåç Dialectales: {}\n",
      "\n",
      "üß™ Caso de prueba 2:\n",
      "Entrada: achachilla. s. Relig. Apacheta. || Ec: Veneraci√≥n de los accidentes geogr√°ficos,...\n",
      "   Acepci√≥n 1:\n",
      "     üè∑Ô∏è  Lema: 'achachilla'\n",
      "     üìù Definici√≥n: 'Apacheta'\n",
      "     üîÑ Sin√≥nimos: []\n",
      "     üåç Dialectales: {}\n",
      "   Acepci√≥n 2:\n",
      "     üè∑Ô∏è  Lema: 'achachilla'\n",
      "     üìù Definici√≥n: ''\n",
      "     üîÑ Sin√≥nimos: ['apachita']\n",
      "     üåç Dialectales: {}\n",
      "=== GENERACI√ìN DE ARCHIVOS JSON ===\n",
      "Utilizando parsers de la Tarea 4\n",
      "\n",
      "‚úÖ Abreviaturas cargadas:\n",
      "   - Categor√≠as gramaticales: 46\n",
      "   - Campos sem√°nticos: 93\n",
      "   - Dialectales: 18\n",
      "\n",
      "üìñ Procesando secci√≥n Quechua-Espa√±ol...\n",
      "Encontradas 9929 entradas potenciales\n",
      "Encontradas 9929 entradas potenciales\n",
      "   ‚úÖ 5001 entradas procesadas\n",
      "\n",
      "üìñ Procesando secci√≥n Espa√±ol-Quechua...\n",
      "Encontradas 4350 entradas potenciales\n",
      "   ‚úÖ 5001 entradas procesadas\n",
      "\n",
      "üìñ Procesando secci√≥n Espa√±ol-Quechua...\n",
      "Encontradas 4350 entradas potenciales\n",
      "   ‚úÖ 5000 entradas procesadas\n",
      "   ‚úÖ 5000 entradas procesadas\n",
      "\n",
      "üíæ Guardado: quechua_espanol.json (5001 entradas)\n",
      "üíæ Guardado: espanol_quechua.json (5000 entradas)\n",
      "\n",
      "‚è±Ô∏è  Tiempo total: 4.1 segundos\n",
      "\n",
      "=== RESUMEN FINAL ===\n",
      "\n",
      "üìö Quechua-Espa√±ol: 5001 entradas\n",
      "   Ejemplo: 'a' ‚Üí 'Indica admiraci√≥n, exclamaci√≥n: ¬°oh!, ¬°ah...'\n",
      "\n",
      "üìö Espa√±ol-Quechua: 5000 entradas\n",
      "   Ejemplo: 'abajo' ‚Üí 'Uray, ura, uran, lurin...'\n",
      "   Dialectales: {'Arg': {'Gen': ['ura']}}\n",
      "\n",
      "üéØ TOTAL: 10001 entradas procesadas correctamente\n",
      "‚úÖ Archivos JSON generados con campos separados seg√∫n especificaciones\n",
      "\n",
      "üéâ PROCESO COMPLETADO EXITOSAMENTE\n",
      "\n",
      "üíæ Guardado: quechua_espanol.json (5001 entradas)\n",
      "üíæ Guardado: espanol_quechua.json (5000 entradas)\n",
      "\n",
      "‚è±Ô∏è  Tiempo total: 4.1 segundos\n",
      "\n",
      "=== RESUMEN FINAL ===\n",
      "\n",
      "üìö Quechua-Espa√±ol: 5001 entradas\n",
      "   Ejemplo: 'a' ‚Üí 'Indica admiraci√≥n, exclamaci√≥n: ¬°oh!, ¬°ah...'\n",
      "\n",
      "üìö Espa√±ol-Quechua: 5000 entradas\n",
      "   Ejemplo: 'abajo' ‚Üí 'Uray, ura, uran, lurin...'\n",
      "   Dialectales: {'Arg': {'Gen': ['ura']}}\n",
      "\n",
      "üéØ TOTAL: 10001 entradas procesadas correctamente\n",
      "‚úÖ Archivos JSON generados con campos separados seg√∫n especificaciones\n",
      "\n",
      "üéâ PROCESO COMPLETADO EXITOSAMENTE\n"
     ]
    }
   ],
   "source": [
    "# TAREA 5: Generaci√≥n de archivos JSON estructurados\n",
    "\n",
    "import time\n",
    "\n",
    "def cargar_abreviaturas():\n",
    "    \"\"\"\n",
    "    Carga las abreviaturas desde el archivo JSON generado anteriormente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"abreviaturas.json\", 'r', encoding='utf-8') as f:\n",
    "            abreviaturas = json.load(f)\n",
    "        return abreviaturas\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: archivo abreviaturas.json no encontrado\")\n",
    "        return {\n",
    "            'categorias_gramaticales': ['s.', 'adj.', 'v.', 'adv.', 'interj.', 'prep.', 'conj.', 'pron.'],\n",
    "            'campos_semanticos': ['Bot.', 'Zool.', 'Med.', 'Hist.', 'Geog.'],\n",
    "            'dialectales': ['Arg.', 'Bol.', 'Pe.Anc.', 'Pe.Aya.', 'Pe.Qos.']\n",
    "        }\n",
    "\n",
    "def generar_archivos_json():\n",
    "    \"\"\"\n",
    "    Ejecuta los parsers y genera los archivos JSON seg√∫n las especificaciones\n",
    "    \"\"\"\n",
    "    print(\"=== GENERACI√ìN DE ARCHIVOS JSON ===\")\n",
    "    print(\"Utilizando parsers de la Tarea 4\\n\")\n",
    "    \n",
    "    # Cargar abreviaturas\n",
    "    abreviaturas = cargar_abreviaturas()\n",
    "    print(f\"‚úÖ Abreviaturas cargadas:\")\n",
    "    print(f\"   - Categor√≠as gramaticales: {len(abreviaturas['categorias_gramaticales'])}\")\n",
    "    print(f\"   - Campos sem√°nticos: {len(abreviaturas['campos_semanticos'])}\")\n",
    "    print(f\"   - Dialectales: {len(abreviaturas['dialectales'])}\")\n",
    "    \n",
    "    inicio = time.time()\n",
    "    \n",
    "    # Crear parsers usando las clases de la Tarea 4\n",
    "    parser_qe = QuechuaEspanolParser(abreviaturas)\n",
    "    parser_eq = EspanolQuechuaParser(abreviaturas)\n",
    "    \n",
    "    # Procesar secci√≥n Quechua-Espa√±ol\n",
    "    entradas_qe = []\n",
    "    if os.path.exists(\"seccion_quechua_espanol.txt\"):\n",
    "        print(\"\\nüìñ Procesando secci√≥n Quechua-Espa√±ol...\")\n",
    "        with open(\"seccion_quechua_espanol.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_qe = f.read()\n",
    "        \n",
    "        entradas_qe = parser_qe.parsear_seccion(texto_qe, max_entradas=5000)\n",
    "        print(f\"   ‚úÖ {len(entradas_qe)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Archivo seccion_quechua_espanol.txt no encontrado\")\n",
    "    \n",
    "    # Procesar secci√≥n Espa√±ol-Quechua\n",
    "    entradas_eq = []\n",
    "    if os.path.exists(\"seccion_espanol_quechua.txt\"):\n",
    "        print(\"\\nüìñ Procesando secci√≥n Espa√±ol-Quechua...\")\n",
    "        with open(\"seccion_espanol_quechua.txt\", 'r', encoding='utf-8') as f:\n",
    "            texto_eq = f.read()\n",
    "        \n",
    "        entradas_eq = parser_eq.parsear_seccion(texto_eq, max_entradas=5000)\n",
    "        print(f\"   ‚úÖ {len(entradas_eq)} entradas procesadas\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Archivo seccion_espanol_quechua.txt no encontrado\")\n",
    "    \n",
    "    # Guardar archivos JSON\n",
    "    if entradas_qe:\n",
    "        with open(\"quechua_espanol.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_qe, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nüíæ Guardado: quechua_espanol.json ({len(entradas_qe)} entradas)\")\n",
    "    \n",
    "    if entradas_eq:\n",
    "        with open(\"espanol_quechua.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(entradas_eq, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Guardado: espanol_quechua.json ({len(entradas_eq)} entradas)\")\n",
    "    \n",
    "    tiempo_total = time.time() - inicio\n",
    "    print(f\"\\n‚è±Ô∏è  Tiempo total: {tiempo_total:.1f} segundos\")\n",
    "    \n",
    "    return entradas_qe, entradas_eq\n",
    "\n",
    "def test_casos_problematicos():\n",
    "    \"\"\"\n",
    "    Prueba los casos espec√≠ficos mencionados por el usuario\n",
    "    \"\"\"\n",
    "    print(\"\\n=== TEST DE CASOS PROBLEM√ÅTICOS ===\")\n",
    "    \n",
    "    abreviaturas = cargar_abreviaturas()\n",
    "    \n",
    "    casos_test = [\n",
    "        \"achacha. s. Juguete. SIN√ìN: pukllana. Pe.Aya: pujllana. Arg: achala, achocha. Bol: pukllana, phukllana. || Vestido lujoso.\",\n",
    "        \"achachilla. s. Relig. Apacheta. || Ec: Veneraci√≥n de los accidentes geogr√°ficos, considerados como lugares sagrados. SIN√ìN: apachita.\"\n",
    "    ]\n",
    "    \n",
    "    for i, caso in enumerate(casos_test, 1):\n",
    "        print(f\"\\nüß™ Caso de prueba {i}:\")\n",
    "        print(f\"Entrada: {caso[:80]}...\")\n",
    "        \n",
    "        resultados = parsear_entrada_ultra_estricta(caso, abreviaturas)\n",
    "        for j, resultado in enumerate(resultados, 1):\n",
    "            print(f\"   Acepci√≥n {j}:\")\n",
    "            print(f\"     üè∑Ô∏è  Lema: '{resultado['lema']}'\")\n",
    "            print(f\"     üìù Definici√≥n: '{resultado['definicion']}'\")\n",
    "            print(f\"     üîÑ Sin√≥nimos: {resultado['sinonimos']}\")\n",
    "            print(f\"     üåç Dialectales: {resultado['variantes_dialectales']}\")\n",
    "\n",
    "def mostrar_resumen_final(entradas_qe, entradas_eq):\n",
    "    \"\"\"\n",
    "    Muestra un resumen final de los resultados\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RESUMEN FINAL ===\")\n",
    "    \n",
    "    if entradas_qe:\n",
    "        print(f\"\\nüìö Quechua-Espa√±ol: {len(entradas_qe)} entradas\")\n",
    "        # Mostrar muestra de la primera entrada\n",
    "        if entradas_qe:\n",
    "            primera = entradas_qe[0]\n",
    "            print(f\"   Ejemplo: '{primera['lema']}' ‚Üí '{primera['definicion'][:50]}...'\")\n",
    "            if primera['variantes_dialectales']:\n",
    "                print(f\"   Dialectales: {primera['variantes_dialectales']}\")\n",
    "    \n",
    "    if entradas_eq:\n",
    "        print(f\"\\nüìö Espa√±ol-Quechua: {len(entradas_eq)} entradas\")\n",
    "        # Mostrar muestra de la primera entrada\n",
    "        if entradas_eq:\n",
    "            primera = entradas_eq[0]\n",
    "            print(f\"   Ejemplo: '{primera['lema']}' ‚Üí '{primera['definicion'][:50]}...'\")\n",
    "            if primera['variantes_dialectales']:\n",
    "                print(f\"   Dialectales: {primera['variantes_dialectales']}\")\n",
    "    \n",
    "    total = len(entradas_qe) + len(entradas_eq)\n",
    "    print(f\"\\nüéØ TOTAL: {total} entradas procesadas correctamente\")\n",
    "    print(\"‚úÖ Archivos JSON generados con campos separados seg√∫n especificaciones\")\n",
    "\n",
    "# EJECUTAR TODO EL PROCESO\n",
    "print(\"Iniciando generaci√≥n completa...\")\n",
    "\n",
    "# 1. Ejecutar test de casos problem√°ticos\n",
    "test_casos_problematicos()\n",
    "\n",
    "# 2. Generar archivos JSON\n",
    "entradas_qe, entradas_eq = generar_archivos_json()\n",
    "\n",
    "# 3. Mostrar resumen final\n",
    "if entradas_qe or entradas_eq:\n",
    "    mostrar_resumen_final(entradas_qe, entradas_eq)\n",
    "    print(\"\\nüéâ PROCESO COMPLETADO EXITOSAMENTE\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Error en el proceso - verificar archivos de entrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d32af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAREA 6 y 7: Validaci√≥n y testing\n",
    "\n",
    "# Importar la librer√≠a desarrollada\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from diccionario_utils import DiccionarioQuechua, cargar_diccionario\n",
    "\n",
    "def ejecutar_tests():\n",
    "    \"\"\"\n",
    "    Ejecuta tests de validaci√≥n de la librer√≠a\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO TESTS DE VALIDACI√ìN ===\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar diccionario\n",
    "        diccionario = cargar_diccionario()\n",
    "        \n",
    "        # Test 1: Estad√≠sticas generales\n",
    "        print(\"1. ESTAD√çSTICAS GENERALES\")\n",
    "        stats = diccionario.estadisticas()\n",
    "        for clave, valor in stats.items():\n",
    "            if isinstance(valor, list) and len(valor) > 5:\n",
    "                print(f\"   {clave}: {len(valor)} elementos\")\n",
    "            elif not isinstance(valor, list):\n",
    "                print(f\"   {clave}: {valor}\")\n",
    "        \n",
    "        # Test 2: B√∫squedas por lemas de prueba\n",
    "        print(\"\\n2. TESTS DE B√öSQUEDA POR LEMAS\")\n",
    "        lemas_prueba = [\"achupalla\", \"puya\", \"qayara\", \"agua\", \"planta\", \n",
    "                       \"hermano\", \"casa\", \"sol\", \"luna\", \"tierra\"]\n",
    "        \n",
    "        for lema in lemas_prueba:\n",
    "            resultado_q = diccionario.buscar_por_quechua(lema)\n",
    "            resultado_e = diccionario.buscar_por_espanol(lema)\n",
    "            \n",
    "            if resultado_q:\n",
    "                print(f\"   ‚úì '{lema}' (Q): {len(resultado_q)} entrada(s)\")\n",
    "                variantes = diccionario.obtener_variantes_dialectales(lema)\n",
    "                if variantes:\n",
    "                    print(f\"     Variantes: {variantes[:3]}...\")\n",
    "            \n",
    "            if resultado_e:\n",
    "                print(f\"   ‚úì '{lema}' (E): {len(resultado_e)} entrada(s)\")\n",
    "        \n",
    "        # Test 3: B√∫squedas por categor√≠as gramaticales\n",
    "        print(\"\\n3. TESTS POR CATEGOR√çAS GRAMATICALES\")\n",
    "        categorias = diccionario.listar_categorias_gramaticales()\n",
    "        for categoria in categorias[:5]:  # Primeras 5 categor√≠as\n",
    "            entradas = diccionario.buscar_por_categoria_gramatical(categoria)\n",
    "            print(f\"   {categoria}: {len(entradas)} entrada(s)\")\n",
    "        \n",
    "        # Test 4: B√∫squedas por campos sem√°nticos\n",
    "        print(\"\\n4. TESTS POR CAMPOS SEM√ÅNTICOS\")\n",
    "        campos = diccionario.listar_campos_semanticos()\n",
    "        for campo in campos[:5]:  # Primeros 5 campos\n",
    "            entradas = diccionario.buscar_por_campo_semantico(campo)\n",
    "            print(f\"   {campo}: {len(entradas)} entrada(s)\")\n",
    "        \n",
    "        # Test 5: Validaci√≥n de estructura de datos\n",
    "        print(\"\\n5. VALIDACI√ìN DE ESTRUCTURA\")\n",
    "        muestra_qe = diccionario.datos_qe[:5]\n",
    "        muestra_eq = diccionario.datos_eq[:5]\n",
    "        \n",
    "        campos_esperados = ['lema', 'categoria_gramatical', 'campo_semantico', \n",
    "                          'definicion', 'variantes_dialectales', 'sinonimos', 'ejemplos']\n",
    "        \n",
    "        print(\"   Campos en entradas Quechua-Espa√±ol:\")\n",
    "        for entrada in muestra_qe:\n",
    "            campos_presentes = [campo for campo in campos_esperados if campo in entrada]\n",
    "            print(f\"     Lema '{entrada.get('lema', 'N/A')}': {len(campos_presentes)}/{len(campos_esperados)} campos\")\n",
    "        \n",
    "        print(\"   Campos en entradas Espa√±ol-Quechua:\")\n",
    "        for entrada in muestra_eq:\n",
    "            campos_presentes = [campo for campo in campos_esperados if campo in entrada]\n",
    "            print(f\"     Lema '{entrada.get('lema', 'N/A')}': {len(campos_presentes)}/{len(campos_esperados)} campos\")\n",
    "        \n",
    "        # Test 6: B√∫squeda de texto completo\n",
    "        print(\"\\n6. TEST DE B√öSQUEDA DE TEXTO COMPLETO\")\n",
    "        texto_prueba = [\"planta\", \"familia\", \"animal\", \"comida\"]\n",
    "        for texto in texto_prueba:\n",
    "            resultados = diccionario.buscar_texto_completo(texto)\n",
    "            print(f\"   '{texto}': {len(resultados)} coincidencia(s)\")\n",
    "        \n",
    "        print(\"\\n=== TESTS COMPLETADOS EXITOSAMENTE ===\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error durante los tests: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar tests\n",
    "if ejecutar_tests():\n",
    "    print(\"\\n‚úì Todos los tests pasaron correctamente\")\n",
    "    print(\"‚úì La librer√≠a diccionario_utils.py est√° funcionando\")\n",
    "    print(\"‚úì Los archivos JSON est√°n correctamente estructurados\")\n",
    "else:\n",
    "    print(\"\\n‚úó Algunos tests fallaron - revisar implementaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3544273",
   "metadata": {},
   "source": [
    "## RESUMEN Y USO DE LA LIBRER√çA\n",
    "\n",
    "### Archivos Generados\n",
    "1. **diccionario_raw.txt** - Texto crudo extra√≠do del PDF\n",
    "2. **abreviaturas.json** - Abreviaturas categorizadas\n",
    "3. **quechua_espanol.json** - Entradas Q‚ÜíE estructuradas\n",
    "4. **espanol_quechua.json** - Entradas E‚ÜíQ estructuradas\n",
    "5. **diccionario_utils.py** - Librer√≠a Python de consultas\n",
    "\n",
    "### Uso de la Librer√≠a\n",
    "\n",
    "```python\n",
    "from diccionario_utils import cargar_diccionario\n",
    "\n",
    "# Cargar diccionario\n",
    "diccionario = cargar_diccionario()\n",
    "\n",
    "# B√∫squedas b√°sicas\n",
    "resultados = diccionario.buscar_por_quechua(\"achupalla\")\n",
    "variantes = diccionario.obtener_variantes_dialectales(\"puya\")\n",
    "\n",
    "# Filtros especializados\n",
    "botanicos = diccionario.buscar_por_campo_semantico(\"Bot.\")\n",
    "sustantivos = diccionario.buscar_por_categoria_gramatical(\"s.\")\n",
    "\n",
    "# Estad√≠sticas\n",
    "stats = diccionario.estadisticas()\n",
    "```\n",
    "\n",
    "### Caracter√≠sticas del Sistema\n",
    "- ‚úÖ **Extracci√≥n autom√°tica** desde PDF\n",
    "- ‚úÖ **Parseo estructurado** con regex optimizadas\n",
    "- ‚úÖ **API de consulta** completa y eficiente\n",
    "- ‚úÖ **Validaci√≥n integrada** con tests automatizados\n",
    "- ‚úÖ **Documentaci√≥n completa** y ejemplos de uso\n",
    "\n",
    "### Impacto\n",
    "Este proyecto sienta las bases para el desarrollo de herramientas de PLN para Quechua, una lengua ind√≠gena hablada por millones de personas en los Andes centrales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
